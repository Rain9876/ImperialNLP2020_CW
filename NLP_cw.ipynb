{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_cw.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJh4PV2irJIK",
        "colab_type": "code",
        "outputId": "b087102d-81f5-4ece-d97b-3a35ab6f7e69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "!pip install pytorch-pretrained-bert"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.4.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.17.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.11.10)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.10 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.14.10)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.3.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.10->boto3->pytorch-pretrained-bert) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.10->boto3->pytorch-pretrained-bert) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.15.0,>=1.14.10->boto3->pytorch-pretrained-bert) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDcxSLX74LD8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.stats.stats import pearsonr\n",
        "from os.path import exists\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
        "import numpy as np\n",
        "from sklearn import linear_model\n",
        "from sklearn.svm import SVR\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.optim.adam\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJ2Jb-Ikq523",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not exists('enzh_data.zip'):\n",
        "    !wget -O enzh_data.zip https://competitions.codalab.org/my/datasets/download/03e23bd7-8084-4542-997b-6a1ca6dd8a5f\n",
        "    !unzip enzh_data.zip\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lE0oDcaGCFOM",
        "colab_type": "code",
        "outputId": "a91418d8-9a2e-47f1-dad4-e909f99ad279",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"---EN-ZH---\")\n",
        "print()\n",
        "\n",
        "with open(\"./train.enzh.src\", \"r\") as enzh_src:\n",
        "  enzh_train_src = enzh_src.readlines()\n",
        "with open(\"./train.enzh.mt\", \"r\",encoding=\"utf-8\") as enzh_mt:\n",
        "  enzh_train_mt = enzh_mt.readlines()\n",
        "with open(\"./train.enzh.scores\", \"r\") as enzh_scores:\n",
        "  enzh_train_scores = enzh_scores.readlines()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---EN-ZH---\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Du132T5GrMGI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"./dev.enzh.src\", \"r\") as enzh_src:\n",
        "  enzh_dev_src = enzh_src.readlines()\n",
        "with open(\"./dev.enzh.mt\", \"r\",encoding=\"utf-8\") as enzh_mt:\n",
        "  enzh_dev_mt = enzh_mt.readlines()\n",
        "with open(\"./dev.enzh.scores\", \"r\") as enzh_scores:\n",
        "  enzh_dev_scores = enzh_scores.readlines()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLOTamyRx2fJ",
        "colab_type": "code",
        "outputId": "cdfb7af7-0c59-4d6d-cccc-1187972a7bb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "use_GPU = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_GPU else \"cpu\")\n",
        "if use_GPU:\n",
        "    torch.cuda.manual_seed(0)\n",
        "print(\"Using GPU: {}\".format(use_GPU))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using GPU: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_p2Rij26PioA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # Example\n",
        "  # marked_text_en = [\n",
        "  #             \"The last conquistador then rides on with his sword drawn.\",\n",
        "  #             \"He shoves Owen into the pit where Digger rips out his son's heart.\",\n",
        "  #             \"Alpha Phi Alpha also participates in the March of Dimes' WalkAmerica and raised over $181,000 in 2006.\",\n",
        "  #             \"In 1995, Deftones released their debut album Adrenaline.\",\n",
        "  #             \"Kyrgios also supports the North Melbourne Kangaroos Football Club in the Australian Football League.\"\n",
        "  # ]\n",
        "  # marked_text_zh = [、\n",
        "  #           \"最后的征服者骑着他的剑继续前进.\",\n",
        "  #           \"他把欧文扔进了挖掘机挖出儿子心脏的坑里.\",\n",
        "  #           \"Alpha Phi Alpha 还参加了 Dimes WalkAmerica 的 3 月活动 ， 并在 2006 年筹集了 181 000 美元。\",\n",
        "  #           \"1995 年 ， Deftones 发行了首张专辑《肾上腺素》。\",\n",
        "  #           \"基尔吉奥斯还在澳大利亚足球联盟中支持北墨尔本袋鼠足球俱乐部.\"\n",
        "  # ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0ZhWdDhv-tG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenization(marked_text_en, marked_text_zh, tokenizer):\n",
        "\n",
        "  indexed_tokens = []\n",
        "  tokenized_text = []\n",
        "  segments_ids = []\n",
        "\n",
        "  for i in range(len(marked_text_en)):\n",
        "\n",
        "    txt = \"[CLS] \"+ marked_text_en[i] +\" [SEP] \" + marked_text_zh[i] + \" [SEP]\"\n",
        "    tokens = tokenizer.tokenize(txt)\n",
        "\n",
        "    tmp = tokens.index(\"[SEP]\")\n",
        "    sep1 = [0]*(tmp+1)\n",
        "    sep2 = [1]*(len(tokens)-tmp - 1)\n",
        "    segments_ids.append(torch.tensor([sep1+sep2]))\n",
        "\n",
        "    tokenized_text.append(tokens)\n",
        "    indexed_tokens.append(torch.tensor([tokenizer.convert_tokens_to_ids(tokens)]))\n",
        "\n",
        "  return indexed_tokens, segments_ids "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlK8YvzrxXUd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bertProcessing(indexed_tokens,segments_ids,model):\n",
        "\n",
        "  sentences_embedding = []\n",
        "  \n",
        "  with torch.no_grad():\n",
        "      \n",
        "      for i in range(len(indexed_tokens)):\n",
        "\n",
        "        # \"encoded_layers\" has shape [12 x 1 x 22 x 768]\n",
        "        encoded_layers, _ = model(indexed_tokens[i].to(device), segments_ids[i].to(device))\n",
        "\n",
        "        # print(\"-\"*30)\n",
        "        # print (\"Number of layers:\", len(encoded_layers))\n",
        "        # layer_i = 0\n",
        "        # print (\"Number of batches:\", len(encoded_layers[layer_i]))\n",
        "        # batch_i = 0\n",
        "        # print (\"Number of tokens:\", len(encoded_layers[layer_i][batch_i]))\n",
        "        # token_i = 0\n",
        "        # print (\"Number of hidden units:\", len(encoded_layers[layer_i][batch_i][token_i]))\n",
        "        \n",
        "        token_embeddings = torch.stack(encoded_layers, dim=0)\n",
        "\n",
        "        token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
        "\n",
        "        token_embeddings = token_embeddings.permute(1,0,2)\n",
        "\n",
        "        # print(token_embeddings.size())    \n",
        "\n",
        "        # \"token_embeddings\" has shape [22 x 12 x 768]\n",
        "        token_vecs_sum = []\n",
        "\n",
        "        for token in token_embeddings:\n",
        "          sum_vec = torch.sum(token[-4:],dim=0)\n",
        "          token_vecs_sum.append(sum_vec)\n",
        "\n",
        "        # \"token_vecs\" is a tensor with shape [22 x 768]\n",
        "        token_vecs = torch.stack(token_vecs_sum,dim=0)\n",
        "\n",
        "        # Calculate the average of all 22 token vectors.        \n",
        "        sentence_embedding = torch.mean(token_vecs, dim=0)\n",
        "\n",
        "        # print(sentence_embedding.size())\n",
        "        sentences_embedding.append(sentence_embedding.cpu().detach().numpy())\n",
        "\n",
        "  return np.array(sentences_embedding)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcdjr0PY3nEj",
        "colab_type": "code",
        "outputId": "04616eb7-5891-44d6-dc51-e347755465bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "model = BertModel.from_pretrained('bert-base-multilingual-cased')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhrPkf0zd5va",
        "colab_type": "code",
        "outputId": "986c97c5-5056-4e44-966a-f52017c74c25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "## Bert Transformer 1: Text1 Delim Text2\n",
        "tokens_train_1, seg_ids_train_1 = tokenization(enzh_train_src,enzh_train_mt,tokenizer)\n",
        "tokens_val_1, seg_ids_val_1 = tokenization(enzh_dev_src,enzh_dev_mt,tokenizer)\n",
        "\n",
        "model_1 = model.to(device)\n",
        "model_1.eval()\n",
        "\n",
        "sentences_embedding_train_1 = bertProcessing(tokens_train_1,seg_ids_train_1,model_1)\n",
        "sentences_embedding_val_1 = bertProcessing(tokens_val_1,seg_ids_val_1,model_1)\n",
        "\n",
        "## Bert Transformer 2: Text2 Delim Text1\n",
        "tokens_train_2, seg_ids_train_2 = tokenization(enzh_train_mt,enzh_train_src,tokenizer)\n",
        "tokens_val_2, seg_ids_val_2 = tokenization(enzh_dev_mt,enzh_dev_src,tokenizer)\n",
        "\n",
        "model_2 = model.to(device)\n",
        "model_2.eval()\n",
        "\n",
        "sentences_embedding_train_2 = bertProcessing(tokens_train_2,seg_ids_train_2,model_2)\n",
        "sentences_embedding_val_2 = bertProcessing(tokens_val_2,seg_ids_val_2,model_2)\n",
        "\n",
        "\n",
        "## Concatenate\n",
        "sentences_embedding_train = []\n",
        "# for i in range(len(sentences_embedding_train_1)):\n",
        "#   sentences_embedding_train.append(np.add((sentences_embedding_train_1[i], sentences_embedding_train_2[i]),axis = 0))\n",
        "# # sentences_embedding_train= [np.array(sentences_embedding_train_1),np.array(sentences_embedding_train_2)]\n",
        "# X_train = np.array(sentences_embedding_train_1)\n",
        "X_train = np.sum([sentences_embedding_train_1,sentences_embedding_train_2],axis=0)\n",
        "\n",
        "sentences_embedding_val = []\n",
        "# for i in range(len(sentences_embedding_val_1)):\n",
        "#   sentences_embedding_val.append(np.add((sentences_embedding_val_1[i], sentences_embedding_val_2[i]),axis=0))\n",
        "# # sentences_embedding_val = [np.array(sentences_embedding_val_1),np.array(sentences_embedding_val_1)]\n",
        "# X_val = np.array(sentences_embedding_val_1)\n",
        "X_val = np.sum([sentences_embedding_val_1, sentences_embedding_val_2],axis = 0)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_val.shape)\n",
        "\n",
        "y_train = np.array(enzh_train_scores).astype(np.float32)\n",
        "y_val = np.array(enzh_dev_scores).astype(np.float32)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7000, 768)\n",
            "(1000, 768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5iUheGuibt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# RMSE\n",
        "def rmse(predictions, targets):\n",
        "    return np.sqrt(((predictions - targets) ** 2).mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRF391ncibaQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SVM\n",
        "for k in ['linear','poly','rbf','sigmoid']:\n",
        "    clf_t = SVR(kernel=k)\n",
        "    clf_t.fit(X_train, y_train)\n",
        "    print(k)\n",
        "    predictions = clf_t.predict(X_val)\n",
        "    pearson = pearsonr(y_val, predictions)\n",
        "    print(f'RMSE: {rmse(predictions,y_val)} Pearson {pearson[0]}')\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfMI9URevf67",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Bayes Regression\n",
        "reg = linear_model.BayesianRidge()\n",
        "reg.fit(X_train, y_train)\n",
        "predictions = reg.predict(X_val)\n",
        "pearson = pearsonr(y_val, predictions)\n",
        "print(f'RMSE: {rmse(predictions,y_val)} Pearson {pearson[0]}')\n",
        "print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swOPVB8glpqu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# FFNN\n",
        "class FeedForwardClassification(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(768,400)\n",
        "        self.fc2 = nn.Linear(400,100)\n",
        "        self.fc3 = nn.Linear(100,1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.fc3(F.relu(self.fc2(F.relu(self.fc1(x)))))\n",
        "        return output\n",
        "\n",
        "# # RNN\n",
        "# class RNNClassification(nn.Module):\n",
        "#     def __init__(self, embedding, hiddenStates, target):\n",
        "#         super().__init__()\n",
        "#         self.lstm1 = nn.LSTM(embedding,hiddenStates) \n",
        "#         self.fc = nn.Linear(hiddenStates,target)\n",
        "#\n",
        "#     def forward(self,embedding)\n",
        "#           # embedding shape(seq_len, batch_size, embedding_size)\n",
        "#           x,_ = F.self.lstm1(embedding)\n",
        "#           # x reshape(embedding) to (seq_len, embedding_size)\n",
        "#           output = self.fc(x.)\n",
        "#           return output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rkr7w1ROqAUw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def training(model,train_loader, criterion, opt):\n",
        "  training_loss = 0\n",
        "  model.train()\n",
        "\n",
        "  for batch_idx,(X_train, y_train) in enumerate(train_loader):\n",
        "    \n",
        "    opt.zero_grad()\n",
        "\n",
        "    X_train = X_train.to(device)\n",
        "    y_train = y_train.to(device)\n",
        "\n",
        "    output = model(X_train)\n",
        "    \n",
        "    loss = criterion(output,y_train)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    opt.step()\n",
        "\n",
        "    training_loss += torch.sqrt(loss).item()\n",
        "\n",
        "    if batch_idx % 200 == 199:\n",
        "      print('[batch: %d]  loss: %.3f'%(batch_idx+1, training_loss/200))\n",
        "      training_loss = 0\n",
        "\n",
        "def testing(model, test_loader, criterion):\n",
        "\n",
        "  model.eval()\n",
        "  predictions = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X_test, y_test in test_loader:\n",
        "      \n",
        "      X_test = X_test.to(device)\n",
        "      \n",
        "      y_test = y_test.to(device)\n",
        "\n",
        "      output = model(X_test)\n",
        "\n",
        "      predictions.append(output.item())\n",
        "    \n",
        "  testing_loss = np.sqrt(((np.array(predictions) - np.array(y_val)) ** 2).mean())\n",
        "  \n",
        "  pearson = pearsonr(y_val, predictions)\n",
        "\n",
        "  print(f'testing_loss: {testing_loss} Pearson {pearson[0]}')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKquPLQiyJjN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def classification():\n",
        "  \n",
        "  train_dat = TensorDataset(torch.tensor(X_train),torch.tensor(y_train))\n",
        "  test_dat = TensorDataset(torch.tensor(X_val),torch.tensor(y_val))\n",
        "\n",
        "  train_loader = DataLoader(train_dat, batch_size=5, shuffle=True,num_workers=2)\n",
        "  val_loader = DataLoader(test_dat, batch_size=1, shuffle=False)\n",
        "\n",
        "  epochs = 25\n",
        "  model = FeedForwardClassification().to(device)\n",
        "  opt = torch.optim.Adam(model.parameters(),lr = 0.0001)\n",
        "  criterion = nn.MSELoss()\n",
        "\n",
        "  for i in range(epochs):\n",
        "    print(\"Epoch: %d\"%(i))\n",
        "    print(\"-\"*30)\n",
        "    training(model,train_loader,criterion,opt)\n",
        "    testing(model,val_loader,criterion)\n",
        "    print(\"-\"*30)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMgLylMN0Kie",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classification()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}