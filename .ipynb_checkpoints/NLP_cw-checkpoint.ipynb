{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PaT9apPrzdgw"
   },
   "source": [
    "# English to Chinese "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "colab_type": "code",
    "id": "oJh4PV2irJIK",
    "outputId": "724639bf-fa17-40d3-c3fc-7c3f59317213"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-pretrained-bert\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
      "\r",
      "\u001b[K     |██▋                             | 10kB 16.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████▎                          | 20kB 1.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████                        | 30kB 2.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▋                     | 40kB 1.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▎                  | 51kB 1.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▉                | 61kB 2.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▌             | 71kB 2.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▏          | 81kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▉        | 92kB 3.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▌     | 102kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▏  | 112kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▊| 122kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 133kB 2.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.11.15)\n",
      "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.4.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.17.5)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.28.1)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.21.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.14.15)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->pytorch-pretrained-bert) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->pytorch-pretrained-bert) (2.6.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.15.0,>=1.14.15->boto3->pytorch-pretrained-bert) (1.12.0)\n",
      "Installing collected packages: pytorch-pretrained-bert\n",
      "Successfully installed pytorch-pretrained-bert-0.6.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-pretrained-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EDcxSLX74LD8"
   },
   "outputs": [],
   "source": [
    "from scipy.stats.stats import pearsonr\n",
    "from os.path import exists\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import SVR\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim.adam\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "colab_type": "code",
    "id": "wJ2Jb-Ikq523",
    "outputId": "d2d21ac2-5c38-40df-c874-fc3e01be57a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-02-27 22:09:04--  https://competitions.codalab.org/my/datasets/download/03e23bd7-8084-4542-997b-6a1ca6dd8a5f\n",
      "Resolving competitions.codalab.org (competitions.codalab.org)... 129.175.22.230\n",
      "Connecting to competitions.codalab.org (competitions.codalab.org)|129.175.22.230|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 FOUND\n",
      "Location: https://newcodalab.lri.fr/prod-private/dataset_data_file/None/630ec/en-zh.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=ff478b541795f4e258beefc47266b43a4566a43ca9e9f63edbf321eba054fa26&X-Amz-Date=20200227T220906Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20200227%2Fnewcodalab%2Fs3%2Faws4_request [following]\n",
      "--2020-02-27 22:09:06--  https://newcodalab.lri.fr/prod-private/dataset_data_file/None/630ec/en-zh.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=ff478b541795f4e258beefc47266b43a4566a43ca9e9f63edbf321eba054fa26&X-Amz-Date=20200227T220906Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20200227%2Fnewcodalab%2Fs3%2Faws4_request\n",
      "Resolving newcodalab.lri.fr (newcodalab.lri.fr)... 129.175.15.11\n",
      "Connecting to newcodalab.lri.fr (newcodalab.lri.fr)|129.175.15.11|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 870893 (850K) [application/zip]\n",
      "Saving to: ‘enzh_data.zip’\n",
      "\n",
      "enzh_data.zip       100%[===================>] 850.48K  1.51MB/s    in 0.5s    \n",
      "\n",
      "2020-02-27 22:09:07 (1.51 MB/s) - ‘enzh_data.zip’ saved [870893/870893]\n",
      "\n",
      "Archive:  enzh_data.zip\n",
      "  inflating: dev.enzh.mt             \n",
      "  inflating: dev.enzh.scores         \n",
      "  inflating: dev.enzh.src            \n",
      "  inflating: test.enzh.mt            \n",
      "  inflating: test.enzh.src           \n",
      "  inflating: train.enzh.mt           \n",
      "  inflating: train.enzh.src          \n",
      "  inflating: train.enzh.scores       \n"
     ]
    }
   ],
   "source": [
    "if not exists('enzh_data.zip'):\n",
    "    !wget -O enzh_data.zip https://competitions.codalab.org/my/datasets/download/03e23bd7-8084-4542-997b-6a1ca6dd8a5f\n",
    "    !unzip enzh_data.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ul0GfjmPyLlA"
   },
   "source": [
    "## Load training, validation and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "Du132T5GrMGI",
    "outputId": "50766a02-104e-4045-d311-90e20ff06fe2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---EN-ZH---\n",
      "\n",
      "training data\n",
      "7000\n",
      "----------\n",
      "dev data\n",
      "1000\n",
      "----------\n",
      "test data\n",
      "1000\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "print(\"---EN-ZH---\")\n",
    "print()\n",
    "\n",
    "print(\"training data\")\n",
    "with open(\"./train.enzh.src\", \"r\") as enzh_src:\n",
    "  enzh_train_src = enzh_src.readlines()\n",
    "with open(\"./train.enzh.mt\", \"r\",encoding=\"utf-8\") as enzh_mt:\n",
    "  enzh_train_mt = enzh_mt.readlines()\n",
    "with open(\"./train.enzh.scores\", \"r\") as enzh_scores:\n",
    "  enzh_train_scores = enzh_scores.readlines()\n",
    "print(len(enzh_train_src))\n",
    "print(\"-\"*10)\n",
    "\n",
    "print(\"dev data\")\n",
    "with open(\"./dev.enzh.src\", \"r\") as enzh_src:\n",
    "  enzh_dev_src = enzh_src.readlines()\n",
    "with open(\"./dev.enzh.mt\", \"r\",encoding=\"utf-8\") as enzh_mt:\n",
    "  enzh_dev_mt = enzh_mt.readlines()\n",
    "with open(\"./dev.enzh.scores\", \"r\") as enzh_scores:\n",
    "  enzh_dev_scores = enzh_scores.readlines()\n",
    "print(len(enzh_dev_src))\n",
    "print(\"-\"*10)\n",
    "\n",
    "print(\"test data\")\n",
    "with open(\"./test.enzh.src\", \"r\") as enzh_src:\n",
    "  enzh_test_src = enzh_src.readlines()\n",
    "with open(\"./test.enzh.mt\", \"r\",encoding=\"utf-8\") as enzh_mt:\n",
    "  enzh_test_mt = enzh_mt.readlines()\n",
    "print(len(enzh_test_src))\n",
    "print(\"-\"*10)\n",
    "\n",
    "# For blind testing only (train & valid combined)\n",
    "# enzh_train_src = enzh_train_src + enzh_dev_src \n",
    "# enzh_train_mt = enzh_train_mt + enzh_dev_mt    \n",
    "# enzh_train_scores = enzh_train_scores + enzh_dev_scores\n",
    "\n",
    "\n",
    "# For training and validation \n",
    "enzh_train_src = enzh_train_src   \n",
    "enzh_train_mt = enzh_train_mt      \n",
    "enzh_train_scores = enzh_train_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jblWBrLIy6YF"
   },
   "source": [
    "## Write testing predicitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U-NruUkJHD9p"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import files\n",
    "from zipfile import ZipFile\n",
    "\n",
    "def writeScores(method_name,scores):\n",
    "  fn = \"predictions.txt\"\n",
    "  print(\"\")\n",
    "  with open(fn, 'w') as output_file:\n",
    "    for idx,x in enumerate(scores):\n",
    "      output_file.write(f\"{x}\\n\")\n",
    "def writeToFile(predictions):\n",
    "  writeScores(\"LSTM\",predictions)\n",
    "  with ZipFile(\"en-zh_lstm.zip\",\"w\") as newzip:\n",
    "\t  newzip.write(\"predictions.txt\")\n",
    "  files.download('en-zh_lstm.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "60cWf8icy8zm"
   },
   "source": [
    "## Load GPU and version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "oLOTamyRx2fJ",
    "outputId": "625b8365-1575-451b-d55e-a0e3ae3a5641"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: Tesla P100-PCIE-16GB\n",
      "Using GPU: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "use_GPU = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_GPU else \"cpu\")\n",
    "print('Device: ' + str(device))\n",
    "if use_GPU:\n",
    "    torch.cuda.manual_seed(0)\n",
    "    print('GPU: ' + str(torch.cuda.get_device_name(int(\"0\")))) \n",
    "print(\"Using GPU: {}\".format(use_GPU))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W62tzi6EzKqw"
   },
   "source": [
    "## Bert Tokenizer\n",
    "Input: Take the english sentences and chinese sentences, with bert tokernizer\\\n",
    "Return:a indexed token and segment id list for combined english and chinese sentence\\\n",
    "The indexed token is the position token and segment id is used to distinguish the position of two sentences. Inside of the tokenization, we give [CLS] and [SEP] to the beginning and end of combined sentence. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z0ZhWdDhv-tG"
   },
   "outputs": [],
   "source": [
    "def tokenization(marked_text_en, marked_text_zh, tokenizer):\n",
    "\n",
    "  indexed_tokens = []\n",
    "  tokenized_text = []\n",
    "  segments_ids = []\n",
    "\n",
    "  for i in range(len(marked_text_en)):\n",
    "\n",
    "    txt = \"[CLS] \"+ marked_text_en[i] +\" [SEP] \" + marked_text_zh[i] + \" [SEP]\"\n",
    "    tokens = tokenizer.tokenize(txt)\n",
    "\n",
    "    tmp = tokens.index(\"[SEP]\")\n",
    "    sep1 = [0]*(tmp+1)\n",
    "    sep2 = [1]*(len(tokens)-tmp - 1)\n",
    "    segments_ids.append(torch.tensor([sep1+sep2]))\n",
    "\n",
    "    tokenized_text.append(tokens)\n",
    "    indexed_tokens.append(torch.tensor([tokenizer.convert_tokens_to_ids(tokens)]))\n",
    "\n",
    "  return indexed_tokens, segments_ids "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G4MgqvoG1Poh"
   },
   "source": [
    "## Bert Pre-Processing\n",
    "Input: indexed tokens, segements id and bert pretrained model\\\n",
    "output: sentence embedding [7000, 768] for training and [1000, 68] for validation\\\n",
    "Inside the pre-preocessing, we sum up the last four encoded layers and get the mean of each word embeddings. The batch size is 1 and N is the number of tokens in the sentences. Bert Base pretrained model generate a embedding dimension of 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zlK8YvzrxXUd"
   },
   "outputs": [],
   "source": [
    "def bertProcessing(indexed_tokens,segments_ids,model):\n",
    "\n",
    "  sentences_embedding = []\n",
    "  \n",
    "  with torch.no_grad():\n",
    "          \n",
    "    for i in range(len(indexed_tokens)):\n",
    "\n",
    "      # \"encoded_layers\" has shape [12 x 1 x N x 768]\n",
    "    \n",
    "      encoded_layers, _ = model(indexed_tokens[i].to(device), segments_ids[i].to(device))\n",
    "\n",
    "      token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "\n",
    "      token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "      token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "      # \"token_embeddings\" has shape [N x 12 x 768]\n",
    "      \n",
    "      token_vecs_sum = []\n",
    "\n",
    "      for token in token_embeddings:\n",
    "        sum_vec = torch.sum(token[-4:],dim=0)\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "\n",
    "      # \"token_vecs\" is a tensor with shape [N x 768]\n",
    "      token_vecs = torch.stack(token_vecs_sum,dim=0)\n",
    "\n",
    "      # Calculate the average of all N token vectors.        \n",
    "      sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "\n",
    "      sentences_embedding.append(sentence_embedding.cpu().detach().numpy())\n",
    "\n",
    "  return np.array(sentences_embedding, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9S1q7_9UmhHO"
   },
   "source": [
    "## Load Model and get embedding\n",
    "We take one single channel, which has the same performance with two channels\\\n",
    "Get the X_train, y_train, X_val, y_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "wcdjr0PY3nEj",
    "outputId": "e7016998-cebd-4e4a-edb6-205ad5a59caf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n",
      "100%|██████████| 995526/995526 [00:00<00:00, 12309597.84B/s]\n",
      "100%|██████████| 662804195/662804195 [00:14<00:00, 44788025.56B/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertModel.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "YhrPkf0zd5va",
    "outputId": "3bcc9e08-e0a6-409b-e299-cc6282cd282a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 768)\n",
      "(1000, 768)\n",
      "(7000,)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "## Bert Transformer 1: Text1 Delim Text2\n",
    "tokens_train_1, seg_ids_train_1 = tokenization(enzh_train_src,enzh_train_mt,tokenizer)\n",
    "tokens_val_1, seg_ids_val_1 = tokenization(enzh_dev_src,enzh_dev_mt,tokenizer)\n",
    "# tokens_test_1,seg_ids_test_1 = tokenization(enzh_test_src,enzh_test_mt,tokenizer)\n",
    "\n",
    "model_1 = model.to(device)\n",
    "model_1.eval()\n",
    "\n",
    "sentences_embedding_train_1 = bertProcessing(tokens_train_1,seg_ids_train_1,model_1)\n",
    "sentences_embedding_val_1 = bertProcessing(tokens_val_1,seg_ids_val_1,model_1)\n",
    "# sentences_embedding_test_1 = bertProcessing(tokens_test_1,seg_ids_test_1,model_1)\n",
    "\n",
    "\n",
    "# ## Bert Transformer 2: Text2 Delim Text1\n",
    "# tokens_train_2, seg_ids_train_2 = tokenization(enzh_train_mt,enzh_train_src,tokenizer)\n",
    "# tokens_val_2, seg_ids_val_2 = tokenization(enzh_dev_mt,enzh_dev_src,tokenizer)\n",
    "# # tokens_test_1,seg_ids_test_2 = tokenization(enzh_test_mt,enzh_test_src,tokenizer)\n",
    "\n",
    "# model_2 = model.to(device)\n",
    "# model_2.eval()\n",
    "\n",
    "# sentences_embedding_train_2 = bertProcessing(tokens_train_2,seg_ids_train_2,model_2)\n",
    "# sentences_embedding_val_2 = bertProcessing(tokens_val_2,seg_ids_val_2,model_2)\n",
    "# ## sentences_embedding_test_2 = bertProcessing(tokens_test_2,seg_ids_test_2,model_2)\n",
    "\n",
    "\n",
    "# ---- train ----\n",
    "## Two channels\n",
    "# X_train = np.sum([sentences_embedding_train_1,sentences_embedding_train_2],axis=0)\n",
    "## Single channel\n",
    "X_train = sentences_embedding_train_1\n",
    "\n",
    "\n",
    "# ---- val -----\n",
    "## Two channels\n",
    "# X_val = np.sum([sentences_embedding_val_1, sentences_embedding_val_2],axis = 0)\n",
    "## Single channel\n",
    "X_val = sentences_embedding_val_1\n",
    "\n",
    "\n",
    "# ---- test -----\n",
    "## Two channels\n",
    "# X_test = np.sum([sentences_embedding_test_1, sentences_embedding_test_2],axis = 0)\n",
    "## Single channels\n",
    "# X_test = sentences_embedding_test_1\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "# print(X_test.shape)\n",
    "\n",
    "## Scores\n",
    "y_train = np.array(enzh_train_scores).astype(np.float32)\n",
    "y_val = np.array(enzh_dev_scores).astype(np.float32)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x2yYmGjinxq2"
   },
   "source": [
    "## Regression models\n",
    "SVM, Random Forest, Bayes Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P5iUheGuibt1"
   },
   "outputs": [],
   "source": [
    "# RMSE\n",
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "oRF391ncibaQ",
    "outputId": "44bd49bc-64c4-4960-e914-693b0d49e51c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear\n",
      "RMSE: 0.8713539078179711 Pearson 0.3887880112807332\n",
      "\n",
      "poly\n",
      "RMSE: 0.8522003820526585 Pearson 0.4461397141553699\n",
      "\n",
      "rbf\n",
      "RMSE: 0.8568153237056716 Pearson 0.44330963777356214\n",
      "\n",
      "sigmoid\n",
      "RMSE: 0.8768037395756224 Pearson 0.4266066166181523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "for k in ['linear','poly','rbf','sigmoid']:\n",
    "    clf_t = SVR(kernel=k)\n",
    "    clf_t.fit(X_train, y_train)\n",
    "    print(k)\n",
    "    predictions = clf_t.predict(X_val)\n",
    "    pearson = pearsonr(y_val, predictions)\n",
    "    print(f'RMSE: {rmse(predictions,y_val)} Pearson {pearson[0]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "7pcQScoMd_Dt",
    "outputId": "4b0f1ddd-7a40-4ab2-f5d5-f377ee8d1799"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.8522009017317218\n",
      "Pearson 0.35397215028205403\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 666)\n",
    "rf.fit(X_train, y_train)\n",
    "predictions = rf.predict(X_val)\n",
    "\n",
    "pearson = pearsonr(y_val, predictions)\n",
    "print('RMSE:', rmse(predictions,y_val))\n",
    "print(f\"Pearson {pearson[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "dfMI9URevf67",
    "outputId": "cca6b04e-3317-4457-b86b-0e4e70ee72c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.8161365821660874 Pearson 0.44338025566197153\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Bayes Regression\n",
    "reg = linear_model.BayesianRidge()\n",
    "reg.fit(X_train, y_train)\n",
    "predictions = reg.predict(X_val)\n",
    "pearson = pearsonr(y_val, predictions)\n",
    "print(f'RMSE: {rmse(predictions,y_val)} Pearson {pearson[0]}')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6NDyK5t7opzW"
   },
   "source": [
    "## Neural Network regression models\n",
    "FFNN and LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "swOPVB8glpqu"
   },
   "outputs": [],
   "source": [
    "# FFNN\n",
    "# Two non-linear layer and one output layer\n",
    "# With ReLu activation function \n",
    "\n",
    "class FFNNRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(768,400)\n",
    "        self.fc2 = nn.Linear(400,100)\n",
    "        self.fc3 = nn.Linear(100,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.fc3(F.relu(self.fc2(F.relu(self.fc1(x)))))\n",
    "        return output\n",
    "\n",
    "# LSTM\n",
    "# Two LSTM layers and one output layer\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, batch_size, output_dim=1,num_layers=2):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers)\n",
    "        self.fc1 = nn.Linear(self.hidden_dim, 1)\n",
    "    \n",
    "    # Initialize the hidden layers\n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        lstm_out, self.hidden = self.lstm(input.view(len(input), self.batch_size, -1))    \n",
    "        y_pred = self.fc1(lstm_out[-1].view(self.batch_size, -1))\n",
    "\n",
    "        return y_pred.view(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T5Tul8KQo9Kr"
   },
   "source": [
    "## Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rkr7w1ROqAUw"
   },
   "outputs": [],
   "source": [
    "def training(model,train_loader, criterion, opt):\n",
    "  training_loss = 0\n",
    "  model.train()\n",
    "\n",
    "  for batch_idx,(X_train, y_train) in enumerate(train_loader):\n",
    "    \n",
    "    ### LSTM only ###\n",
    "    X_train = X_train.view(-1,len(X_train),len(X_train[0]))\n",
    "    #################\n",
    "\n",
    "    opt.zero_grad()\n",
    "\n",
    "    X_train = X_train.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "\n",
    "    output = model(X_train)\n",
    "    \n",
    "    loss = criterion(output,y_train)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    opt.step()\n",
    "\n",
    "    training_loss += torch.sqrt(loss).item()\n",
    "\n",
    "    if batch_idx % 200 == 199:\n",
    "      print('[batch: %d]  loss: %.3f'%(batch_idx+1, training_loss/200))\n",
    "      training_loss = 0\n",
    "\n",
    "def testing(model, test_loader, criterion):\n",
    "\n",
    "  model.eval()\n",
    "  predictions = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for X_test, y_test in test_loader:\n",
    "\n",
    "      ## LSTM only ##\n",
    "      X_test = X_test.view(-1,len(X_test),len(X_test[0]))\n",
    "      ###############\n",
    "      \n",
    "      X_test = X_test.to(device)\n",
    "      \n",
    "      y_test = y_test.to(device)\n",
    "\n",
    "      output = model(X_test)\n",
    "\n",
    "\n",
    "      ## LSTM only ###\n",
    "      for pred in output.data.tolist():\n",
    "        predictions.append(pred)\n",
    "      #################\n",
    "      ## FFNN only ##\n",
    "      # output = sum(output.data.tolist(),[])\n",
    "      # for pred in output:\n",
    "      #   predictions.append(pred)  \n",
    "      #################\n",
    "\n",
    "  testing_loss = np.sqrt(((np.array(predictions) - np.array(y_val)) ** 2).mean())\n",
    "  \n",
    "  pearson = pearsonr(y_val, predictions)\n",
    "\n",
    "  print(f'testing_loss: {testing_loss} Pearson {pearson[0]}')\n",
    "\n",
    "  return predictions, testing_loss, pearson[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dzoFa_KOqago"
   },
   "source": [
    "## Running\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TKquPLQiyJjN"
   },
   "outputs": [],
   "source": [
    "def regression():\n",
    "  bs = 4\n",
    "  epochs = 10\n",
    "  iput_dim = 768  ## For LSTM \n",
    "  hidden_dim = 750  ## For LSTM\n",
    "\n",
    "  predictions = []\n",
    "  testing_loss = []\n",
    "  pearsons = []\n",
    "  \n",
    "  train_dat = TensorDataset(torch.tensor(X_train),torch.tensor(y_train))\n",
    "  test_dat = TensorDataset(torch.tensor(X_val),torch.tensor(y_val))\n",
    "\n",
    "  train_loader = DataLoader(train_dat, batch_size=bs, shuffle=True,num_workers=2)\n",
    "  val_loader = DataLoader(test_dat, batch_size=bs, shuffle=False)\n",
    "\n",
    "  #model = FFNNRegression().to(device)\n",
    "\n",
    "  model = LSTM(iput_dim,hidden_dim,bs).to(device)\n",
    "  \n",
    "  opt = torch.optim.Adam(model.parameters(),lr = 0.0001,weight_decay=1e-6)\n",
    "  \n",
    "  criterion = nn.MSELoss()\n",
    "\n",
    "  for i in range(epochs):\n",
    "    print(\"Epoch: %d\"%(i+1))\n",
    "    print(\"-\"*30)\n",
    "    training(model,train_loader,criterion,opt)\n",
    "    predictions, loss, pearson = testing(model,val_loader,criterion)\n",
    "\n",
    "    testing_loss.append(loss)\n",
    "    pearsons.append(pearson)\n",
    "\n",
    "    print(\"-\"*30)\n",
    "\n",
    "  # Plot the loss and pearsons diagrams\n",
    "  x_epochs = list(range(1,epochs+1))\n",
    "  fig, (ax1,ax2) = plt.subplots(1,2)\n",
    "  ax1.plot(x_epochs, testing_loss)\n",
    "  ax1.set(xlabel='epochs', ylabel='test loss')\n",
    "  ax2.plot(x_epochs, pearsons)\n",
    "  ax2.set(xlabel='epochs', ylabel='test pearsons')\n",
    "  ax1.set_xticks(x_epochs) \n",
    "  ax2.set_xticks(x_epochs) \n",
    "  fig.tight_layout(pad=4.0)\n",
    "  plt.show()\n",
    "  \n",
    "  # writeToFile(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3E-Yt0pQqLZm"
   },
   "source": [
    "## Regression and evaluation on average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Ml-mX7vSAfP_",
    "outputId": "af9ced62-7b03-4f7e-9bd4-41f5302d3cce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "------------------------------\n",
      "[batch: 200]  loss: 0.811\n",
      "[batch: 400]  loss: 0.850\n",
      "[batch: 600]  loss: 0.805\n",
      "[batch: 800]  loss: 0.764\n",
      "[batch: 1000]  loss: 0.800\n",
      "[batch: 1200]  loss: 0.809\n",
      "[batch: 1400]  loss: 0.731\n",
      "[batch: 1600]  loss: 0.771\n",
      "testing_loss: 0.8265029080701928 Pearson 0.43556634047000964\n",
      "------------------------------\n",
      "Epoch: 2\n",
      "------------------------------\n",
      "[batch: 200]  loss: 0.779\n",
      "[batch: 400]  loss: 0.775\n",
      "[batch: 600]  loss: 0.797\n",
      "[batch: 800]  loss: 0.736\n",
      "[batch: 1000]  loss: 0.748\n",
      "[batch: 1200]  loss: 0.725\n",
      "[batch: 1400]  loss: 0.758\n",
      "[batch: 1600]  loss: 0.703\n",
      "testing_loss: 0.8191446963592924 Pearson 0.44235878742191115\n",
      "------------------------------\n",
      "Epoch: 3\n",
      "------------------------------\n",
      "[batch: 200]  loss: 0.748\n",
      "[batch: 400]  loss: 0.700\n",
      "[batch: 600]  loss: 0.705\n",
      "[batch: 800]  loss: 0.735\n",
      "[batch: 1000]  loss: 0.748\n",
      "[batch: 1200]  loss: 0.722\n",
      "[batch: 1400]  loss: 0.725\n",
      "[batch: 1600]  loss: 0.713\n",
      "testing_loss: 0.8124004320393665 Pearson 0.45173866378159677\n",
      "------------------------------\n",
      "Epoch: 4\n",
      "------------------------------\n",
      "[batch: 200]  loss: 0.696\n",
      "[batch: 400]  loss: 0.684\n",
      "[batch: 600]  loss: 0.674\n",
      "[batch: 800]  loss: 0.674\n",
      "[batch: 1000]  loss: 0.733\n",
      "[batch: 1200]  loss: 0.690\n",
      "[batch: 1400]  loss: 0.741\n",
      "[batch: 1600]  loss: 0.702\n",
      "testing_loss: 0.8471663064253935 Pearson 0.45086665020483235\n",
      "------------------------------\n",
      "Epoch: 5\n",
      "------------------------------\n",
      "[batch: 200]  loss: 0.673\n",
      "[batch: 400]  loss: 0.647\n",
      "[batch: 600]  loss: 0.609\n",
      "[batch: 800]  loss: 0.663\n",
      "[batch: 1000]  loss: 0.684\n",
      "[batch: 1200]  loss: 0.667\n",
      "[batch: 1400]  loss: 0.633\n",
      "[batch: 1600]  loss: 0.629\n",
      "testing_loss: 0.8475642115557317 Pearson 0.43571377615375806\n",
      "------------------------------\n",
      "Epoch: 6\n",
      "------------------------------\n",
      "[batch: 200]  loss: 0.583\n",
      "[batch: 400]  loss: 0.619\n",
      "[batch: 600]  loss: 0.594\n",
      "[batch: 800]  loss: 0.609\n",
      "[batch: 1000]  loss: 0.605\n",
      "[batch: 1200]  loss: 0.600\n",
      "[batch: 1400]  loss: 0.578\n",
      "[batch: 1600]  loss: 0.586\n",
      "testing_loss: 0.8527028482449611 Pearson 0.41674745954734677\n",
      "------------------------------\n",
      "Epoch: 7\n",
      "------------------------------\n",
      "[batch: 200]  loss: 0.524\n",
      "[batch: 400]  loss: 0.530\n",
      "[batch: 600]  loss: 0.582\n",
      "[batch: 800]  loss: 0.521\n",
      "[batch: 1000]  loss: 0.541\n",
      "[batch: 1200]  loss: 0.526\n",
      "[batch: 1400]  loss: 0.519\n",
      "[batch: 1600]  loss: 0.531\n",
      "testing_loss: 0.8641180976630648 Pearson 0.3937126262553863\n",
      "------------------------------\n",
      "Epoch: 8\n",
      "------------------------------\n",
      "[batch: 200]  loss: 0.456\n",
      "[batch: 400]  loss: 0.495\n",
      "[batch: 600]  loss: 0.467\n",
      "[batch: 800]  loss: 0.471\n",
      "[batch: 1000]  loss: 0.485\n",
      "[batch: 1200]  loss: 0.464\n",
      "[batch: 1400]  loss: 0.485\n",
      "[batch: 1600]  loss: 0.488\n",
      "testing_loss: 0.8764960094623072 Pearson 0.3946334582815274\n",
      "------------------------------\n",
      "Epoch: 9\n",
      "------------------------------\n",
      "[batch: 200]  loss: 0.400\n",
      "[batch: 400]  loss: 0.418\n",
      "[batch: 600]  loss: 0.421\n",
      "[batch: 800]  loss: 0.405\n",
      "[batch: 1000]  loss: 0.415\n",
      "[batch: 1200]  loss: 0.419\n",
      "[batch: 1400]  loss: 0.447\n",
      "[batch: 1600]  loss: 0.424\n",
      "testing_loss: 0.9215378241009178 Pearson 0.3846623117438215\n",
      "------------------------------\n",
      "Epoch: 10\n",
      "------------------------------\n",
      "[batch: 200]  loss: 0.360\n",
      "[batch: 400]  loss: 0.348\n",
      "[batch: 600]  loss: 0.350\n",
      "[batch: 800]  loss: 0.373\n",
      "[batch: 1000]  loss: 0.370\n",
      "[batch: 1200]  loss: 0.380\n",
      "[batch: 1400]  loss: 0.400\n",
      "[batch: 1600]  loss: 0.395\n",
      "testing_loss: 0.8935538447224335 Pearson 0.3887500172955388\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAADeCAYAAADy3YFwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "regression()\n",
    "\n",
    "\n",
    "## For 5 times training \n",
    "# Evaluate average loss and pearsons\n",
    "# With epoch 4\n",
    "\n",
    "# losses = []\n",
    "# pears = []\n",
    "# for i in range(5):\n",
    "#   loss, pear = regression()\n",
    "#   losses.append(loss[3])\n",
    "#   pears.append(pear[3])\n",
    "# print(f\"Average test loss:{sum(losses)/len(losses)} Average pearsons: {sum(pears)/len(pears)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "24quEuDXwnMX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "NLP_cw.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
