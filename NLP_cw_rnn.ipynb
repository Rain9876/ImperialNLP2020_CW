{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_cw_rnn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJh4PV2irJIK",
        "colab_type": "code",
        "outputId": "3585f441-0ec1-4422-db72-4418efa6b1c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "!pip install pytorch-pretrained-bert"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-pretrained-bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 19.5MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 3.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 4.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 4.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 4.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 4.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 4.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.17.5)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.11.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.28.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.21.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.14.15)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->pytorch-pretrained-bert) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->pytorch-pretrained-bert) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.15.0,>=1.14.15->boto3->pytorch-pretrained-bert) (1.12.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDcxSLX74LD8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.stats.stats import pearsonr\n",
        "from os.path import exists\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
        "import numpy as np\n",
        "from sklearn import linear_model\n",
        "from sklearn.svm import SVR\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.optim.adam\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJ2Jb-Ikq523",
        "colab_type": "code",
        "outputId": "3d6319b6-2ea2-4710-b5bd-062b0a5a74d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "if not exists('enzh_data.zip'):\n",
        "    !wget -O enzh_data.zip https://competitions.codalab.org/my/datasets/download/03e23bd7-8084-4542-997b-6a1ca6dd8a5f\n",
        "    !unzip enzh_data.zip\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-23 09:36:01--  https://competitions.codalab.org/my/datasets/download/03e23bd7-8084-4542-997b-6a1ca6dd8a5f\n",
            "Resolving competitions.codalab.org (competitions.codalab.org)... 129.175.22.230\n",
            "Connecting to competitions.codalab.org (competitions.codalab.org)|129.175.22.230|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: https://newcodalab.lri.fr/prod-private/dataset_data_file/None/630ec/en-zh.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=288af897d984970f664e878a2378c0776b1e1525f657cf7e8c3e95dba4559fd2&X-Amz-Date=20200223T093602Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20200223%2Fnewcodalab%2Fs3%2Faws4_request [following]\n",
            "--2020-02-23 09:36:02--  https://newcodalab.lri.fr/prod-private/dataset_data_file/None/630ec/en-zh.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=288af897d984970f664e878a2378c0776b1e1525f657cf7e8c3e95dba4559fd2&X-Amz-Date=20200223T093602Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20200223%2Fnewcodalab%2Fs3%2Faws4_request\n",
            "Resolving newcodalab.lri.fr (newcodalab.lri.fr)... 129.175.15.11\n",
            "Connecting to newcodalab.lri.fr (newcodalab.lri.fr)|129.175.15.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 870893 (850K) [application/zip]\n",
            "Saving to: ‘enzh_data.zip’\n",
            "\n",
            "enzh_data.zip       100%[===================>] 850.48K  1.00MB/s    in 0.8s    \n",
            "\n",
            "2020-02-23 09:36:04 (1.00 MB/s) - ‘enzh_data.zip’ saved [870893/870893]\n",
            "\n",
            "Archive:  enzh_data.zip\n",
            "  inflating: dev.enzh.mt             \n",
            "  inflating: dev.enzh.scores         \n",
            "  inflating: dev.enzh.src            \n",
            "  inflating: test.enzh.mt            \n",
            "  inflating: test.enzh.src           \n",
            "  inflating: train.enzh.mt           \n",
            "  inflating: train.enzh.src          \n",
            "  inflating: train.enzh.scores       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Du132T5GrMGI",
        "colab_type": "code",
        "outputId": "55834c3b-8bc8-4046-fc76-520807c96df0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "print(\"---EN-ZH---\")\n",
        "print()\n",
        "\n",
        "print(\"training data\")\n",
        "with open(\"./train.enzh.src\", \"r\") as enzh_src:\n",
        "  enzh_train_src = enzh_src.readlines()\n",
        "with open(\"./train.enzh.mt\", \"r\",encoding=\"utf-8\") as enzh_mt:\n",
        "  enzh_train_mt = enzh_mt.readlines()\n",
        "with open(\"./train.enzh.scores\", \"r\") as enzh_scores:\n",
        "  enzh_train_scores = enzh_scores.readlines()\n",
        "print(len(enzh_train_src))\n",
        "print(\"-\"*10)\n",
        "print(\"dev data\")\n",
        "with open(\"./dev.enzh.src\", \"r\") as enzh_src:\n",
        "  enzh_dev_src = enzh_src.readlines()\n",
        "with open(\"./dev.enzh.mt\", \"r\",encoding=\"utf-8\") as enzh_mt:\n",
        "  enzh_dev_mt = enzh_mt.readlines()\n",
        "with open(\"./dev.enzh.scores\", \"r\") as enzh_scores:\n",
        "  enzh_dev_scores = enzh_scores.readlines()\n",
        "print(len(enzh_dev_src))\n",
        "print(\"-\"*10)\n",
        "print(\"test data\")\n",
        "with open(\"./test.enzh.src\", \"r\") as enzh_src:\n",
        "  enzh_test_src = enzh_src.readlines()\n",
        "with open(\"./test.enzh.mt\", \"r\",encoding=\"utf-8\") as enzh_mt:\n",
        "  enzh_test_mt = enzh_mt.readlines()\n",
        "\n",
        "print(len(enzh_dev_src))\n",
        "print(\"-\"*10)\n",
        "\n",
        "# For testing (train & valid combined)\n",
        "# enzh_train_src = enzh_train_src + enzh_dev_src \n",
        "# enzh_train_mt = enzh_train_mt + enzh_dev_mt    \n",
        "# enzh_train_scores = enzh_train_scores + enzh_dev_scores\n",
        "\n",
        "\n",
        "# For training and validation\n",
        "enzh_train_src = enzh_train_src   \n",
        "enzh_train_mt = enzh_train_mt      \n",
        "enzh_train_scores = enzh_train_scores\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---EN-ZH---\n",
            "\n",
            "training data\n",
            "7000\n",
            "----------\n",
            "dev data\n",
            "1000\n",
            "----------\n",
            "test data\n",
            "1000\n",
            "----------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-NruUkJHD9p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "from zipfile import ZipFile\n",
        "\n",
        "def writeScores(method_name,scores):\n",
        "  fn = \"predictions.txt\"\n",
        "  print(\"\")\n",
        "  with open(fn, 'w') as output_file:\n",
        "    for idx,x in enumerate(scores):\n",
        "      output_file.write(f\"{x}\\n\")\n",
        "def writeToFile(predictions):\n",
        "  writeScores(\"LSTM\",predictions)\n",
        "  with ZipFile(\"en-zh_lstm.zip\",\"w\") as newzip:\n",
        "\t  newzip.write(\"predictions.txt\")\n",
        "  files.download('en-zh_lstm.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLOTamyRx2fJ",
        "colab_type": "code",
        "outputId": "1476a1e9-0060-47db-d385-67b27fb7e3a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import torch\n",
        "use_GPU = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_GPU else \"cpu\")\n",
        "print('Device: ' + str(device))\n",
        "if use_GPU:\n",
        "    torch.cuda.manual_seed(0)\n",
        "    print('GPU: ' + str(torch.cuda.get_device_name(int(\"0\")))) \n",
        "print(\"Using GPU: {}\".format(use_GPU))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "GPU: Tesla P100-PCIE-16GB\n",
            "Using GPU: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_p2Rij26PioA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # Example\n",
        "  # marked_text_en = [\n",
        "  #             \"The last conquistador then rides on with his sword drawn.\",\n",
        "  #             \"He shoves Owen into the pit where Digger rips out his son's heart.\",\n",
        "  #             \"Alpha Phi Alpha also participates in the March of Dimes' WalkAmerica and raised over $181,000 in 2006.\",\n",
        "  #             \"In 1995, Deftones released their debut album Adrenaline.\",\n",
        "  #             \"Kyrgios also supports the North Melbourne Kangaroos Football Club in the Australian Football League.\"\n",
        "  # ]\n",
        "  # marked_text_zh = [、\n",
        "  #           \"最后的征服者骑着他的剑继续前进.\",\n",
        "  #           \"他把欧文扔进了挖掘机挖出儿子心脏的坑里.\",\n",
        "  #           \"Alpha Phi Alpha 还参加了 Dimes WalkAmerica 的 3 月活动 ， 并在 2006 年筹集了 181 000 美元。\",\n",
        "  #           \"1995 年 ， Deftones 发行了首张专辑《肾上腺素》。\",\n",
        "  #           \"基尔吉奥斯还在澳大利亚足球联盟中支持北墨尔本袋鼠足球俱乐部.\"\n",
        "  # ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0ZhWdDhv-tG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenization(marked_text_en, marked_text_zh, tokenizer):\n",
        "\n",
        "  indexed_tokens = []\n",
        "  tokenized_text = []\n",
        "  segments_ids = []\n",
        "\n",
        "  for i in range(len(marked_text_en)):\n",
        "\n",
        "    txt = \"[CLS] \"+ marked_text_en[i] +\" [SEP] \" + marked_text_zh[i] + \" [SEP]\"\n",
        "    tokens = tokenizer.tokenize(txt)\n",
        "\n",
        "    tmp = tokens.index(\"[SEP]\")\n",
        "    sep1 = [0]*(tmp+1)\n",
        "    sep2 = [1]*(len(tokens)-tmp - 1)\n",
        "    segments_ids.append(torch.tensor([sep1+sep2]))\n",
        "\n",
        "    tokenized_text.append(tokens)\n",
        "    indexed_tokens.append(torch.tensor([tokenizer.convert_tokens_to_ids(tokens)]))\n",
        "\n",
        "  return indexed_tokens, segments_ids "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlK8YvzrxXUd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bertProcessing(indexed_tokens,segments_ids,model):\n",
        "\n",
        "  sentences_embedding = []\n",
        "  \n",
        "  with torch.no_grad():\n",
        "          \n",
        "    for i in range(len(indexed_tokens)):\n",
        "\n",
        "      # \"encoded_layers\" has shape [12 x 1 x 22 x 768]\n",
        "      encoded_layers, _ = model(indexed_tokens[i].to(device), segments_ids[i].to(device))\n",
        "\n",
        "      # print(\"-\"*30)\n",
        "      # print (\"Number of layers:\", len(encoded_layers))\n",
        "      # layer_i = 0\n",
        "      # print (\"Number of batches:\", len(encoded_layers[layer_i]))\n",
        "      # batch_i = 0\n",
        "      # print (\"Number of tokens:\", len(encoded_layers[layer_i][batch_i]))\n",
        "      # token_i = 0\n",
        "      # print (\"Number of hidden units:\", len(encoded_layers[layer_i][batch_i][token_i]))\n",
        "      \n",
        "      token_embeddings = torch.stack(encoded_layers, dim=0)\n",
        "\n",
        "      token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
        "\n",
        "      token_embeddings = token_embeddings.permute(1,0,2)\n",
        "\n",
        "      # print(token_embeddings.size())    \n",
        "\n",
        "      # \"token_embeddings\" has shape [22 x 12 x 768]\n",
        "      \n",
        "      token_vecs_sum = []\n",
        "\n",
        "      for token in token_embeddings:\n",
        "        sum_vec = torch.sum(token[-4:],dim=0)\n",
        "        token_vecs_sum.append(sum_vec)\n",
        "\n",
        "      # \"token_vecs\" is a tensor with shape [22 x 768]\n",
        "      token_vecs = torch.stack(token_vecs_sum,dim=0)\n",
        "\n",
        "      # Calculate the average of all 22 token vectors.        \n",
        "      sentence_embedding = torch.mean(token_vecs, dim=0)\n",
        "\n",
        "      # print(sentence_embedding.size())\n",
        "      sentences_embedding.append(sentence_embedding.cpu().detach().numpy())\n",
        "\n",
        "  return np.array(sentences_embedding)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcdjr0PY3nEj",
        "colab_type": "code",
        "outputId": "6e7525ce-e3be-4fdd-adec-0f632f52f629",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "model = BertModel.from_pretrained('bert-base-multilingual-cased')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n",
            "100%|██████████| 995526/995526 [00:00<00:00, 2526620.60B/s]\n",
            "100%|██████████| 662804195/662804195 [00:17<00:00, 38740591.86B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhrPkf0zd5va",
        "colab_type": "code",
        "outputId": "1803e326-f50f-4960-cfe8-1d4cadd3f595",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "## Bert Transformer 1: Text1 Delim Text2\n",
        "tokens_train_1, seg_ids_train_1 = tokenization(enzh_train_src,enzh_train_mt,tokenizer)\n",
        "tokens_val_1, seg_ids_val_1 = tokenization(enzh_dev_src,enzh_dev_mt,tokenizer)\n",
        "# tokens_test_1,seg_ids_test_1 = tokenization(enzh_test_src,enzh_test_mt,tokenizer)\n",
        "\n",
        "model_1 = model.to(device)\n",
        "model_1.eval()\n",
        "\n",
        "sentences_embedding_train_1 = bertProcessing(tokens_train_1,seg_ids_train_1,model_1)\n",
        "sentences_embedding_val_1 = bertProcessing(tokens_val_1,seg_ids_val_1,model_1)\n",
        "# sentences_embedding_test_1 = bertProcessing(tokens_test_1,seg_ids_test_1,model_1)\n",
        "\n",
        "\n",
        "## Bert Transformer 2: Text2 Delim Text1\n",
        "tokens_train_2, seg_ids_train_2 = tokenization(enzh_train_mt,enzh_train_src,tokenizer)\n",
        "tokens_val_2, seg_ids_val_2 = tokenization(enzh_dev_mt,enzh_dev_src,tokenizer)\n",
        "# tokens_test_1,seg_ids_test_2 = tokenization(enzh_test_mt,enzh_test_src,tokenizer)\n",
        "\n",
        "model_2 = model.to(device)\n",
        "model_2.eval()\n",
        "\n",
        "sentences_embedding_train_2 = bertProcessing(tokens_train_2,seg_ids_train_2,model_2)\n",
        "sentences_embedding_val_2 = bertProcessing(tokens_val_2,seg_ids_val_2,model_2)\n",
        "## sentences_embedding_test_2 = bertProcessing(tokens_test_2,seg_ids_test_2,model_2)\n",
        "\n",
        "\n",
        "# ---- train ----\n",
        "## Concatenate\n",
        "# sentences_embedding_train = []\n",
        "# for i in range(len(sentences_embedding_train_1)):\n",
        "#   sentences_embedding_train.append(np.add((sentences_embedding_train_1[i], sentences_embedding_train_2[i]),axis = 0))\n",
        "# # sentences_embedding_train= [np.array(sentences_embedding_train_1),np.array(sentences_embedding_train_2)]\n",
        "# X_train = np.array(sentences_embedding_train_1)\n",
        "\n",
        "## Sum\n",
        "X_train = np.sum([sentences_embedding_train_1,sentences_embedding_train_2],axis=0)\n",
        "\n",
        "## Single\n",
        "# X_train = sentences_embedding_train_1\n",
        "\n",
        "# ---- val -----\n",
        "## Concatenate\n",
        "# sentences_embedding_val = []\n",
        "# for i in range(len(sentences_embedding_val_1)):\n",
        "#   sentences_embedding_val.append(np.add((sentences_embedding_val_1[i], sentences_embedding_val_2[i]),axis=0))\n",
        "# # sentences_embedding_val = [np.array(sentences_embedding_val_1),np.array(sentences_embedding_val_1)]\n",
        "# X_val = np.array(sentences_embedding_val_1)\n",
        "\n",
        "## Sum\n",
        "X_val = np.sum([sentences_embedding_val_1, sentences_embedding_val_2],axis = 0)\n",
        "\n",
        "## Single\n",
        "# X_val = sentences_embedding_val_1\n",
        "\n",
        "# ---- test -----\n",
        "## Concatenate\n",
        "\n",
        "## Sum\n",
        "# X_test = np.sum([sentences_embedding_test_1, sentences_embedding_test_2],axis = 0)\n",
        "\n",
        "## Single\n",
        "# X_test = sentences_embedding_test_1\n",
        "\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_val.shape)\n",
        "# print(X_test.shape)\n",
        "\n",
        "## Scores\n",
        "y_train = np.array(enzh_train_scores).astype(np.float32)\n",
        "y_val = np.array(enzh_dev_scores).astype(np.float32)\n",
        "\n",
        "print(y_train.shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7000, 768)\n",
            "(1000, 768)\n",
            "(7000,)\n",
            "(1000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5iUheGuibt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# RMSE\n",
        "def rmse(predictions, targets):\n",
        "    return np.sqrt(((predictions - targets) ** 2).mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRF391ncibaQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SVM\n",
        "for k in ['linear','poly','rbf','sigmoid']:\n",
        "    clf_t = SVR(kernel=k)\n",
        "    clf_t.fit(X_train, y_train)\n",
        "    print(k)\n",
        "    predictions = clf_t.predict(X_val)\n",
        "    pearson = pearsonr(y_val, predictions)\n",
        "    print(f'RMSE: {rmse(predictions,y_val)} Pearson {pearson[0]}')\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pcQScoMd_Dt",
        "colab_type": "code",
        "outputId": "4b0f1ddd-7a40-4ab2-f5d5-f377ee8d1799",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Random Forest\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "rf = RandomForestRegressor(n_estimators = 1000, random_state = 666)\n",
        "rf.fit(X_train, y_train)\n",
        "predictions = rf.predict(X_val)\n",
        "\n",
        "pearson = pearsonr(y_val, predictions)\n",
        "print('RMSE:', rmse(predictions,y_val))\n",
        "print(f\"Pearson {pearson[0]}\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE: 0.8522009017317218\n",
            "Pearson 0.35397215028205403\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfMI9URevf67",
        "colab_type": "code",
        "outputId": "1e5de8bd-f545-432b-e141-f284174ae7ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Bayes Regression\n",
        "reg = linear_model.BayesianRidge()\n",
        "reg.fit(X_train, y_train)\n",
        "predictions = reg.predict(X_val)\n",
        "pearson = pearsonr(y_val, predictions)\n",
        "print(f'RMSE: {rmse(predictions,y_val)} Pearson {pearson[0]}')\n",
        "print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE: 0.8161074710861961 Pearson 0.44346961729217305\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swOPVB8glpqu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# FFNN\n",
        "class FeedForwardClassification(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(768,400)\n",
        "        self.fc2 = nn.Linear(400,100)\n",
        "        self.fc3 = nn.Linear(100,1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.fc3(F.relu(self.fc2(F.relu(self.fc1(x)))))\n",
        "        return output\n",
        "\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, batch_size, output_dim=1,num_layers=2):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.batch_size = batch_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers)\n",
        "        self.fc1 = nn.Linear(self.hidden_dim, 1)\n",
        "        self.fc2 = nn.Linear(50,output_dim)\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
        "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
        "\n",
        "    def forward(self, input):\n",
        "        # Forward pass through LSTM layer\n",
        "        # shape of lstm_out: [input_size, batch_size, hidden_dim]\n",
        "        # shape of self.hidden: (a, b), where a and b both \n",
        "        # have shape (num_layers, batch_size, hidden_dim).\n",
        "        lstm_out, self.hidden = self.lstm(input.view(len(input), self.batch_size, -1))\n",
        "        \n",
        "        # Only take the output from the final timetep\n",
        "        # Can pass on the entirety of lstm_out to the next layer if it is a seq2seq prediction\n",
        "        y_pred = self.fc1(lstm_out[-1].view(self.batch_size, -1))\n",
        "\n",
        "        # y_pred = self.fc2(y_pred)\n",
        "\n",
        "        return y_pred.view(-1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rkr7w1ROqAUw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def training(model,train_loader, criterion, opt):\n",
        "  training_loss = 0\n",
        "  model.train()\n",
        "\n",
        "  for batch_idx,(X_train, y_train) in enumerate(train_loader):\n",
        "    \n",
        "\n",
        "    X_train = X_train.view(-1,len(X_train),len(X_train[0]))\n",
        "\n",
        "    opt.zero_grad()\n",
        "\n",
        "    X_train = X_train.to(device)\n",
        "    y_train = y_train.to(device)\n",
        "\n",
        "    output = model(X_train)\n",
        "    \n",
        "    loss = criterion(output,y_train)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    opt.step()\n",
        "\n",
        "    training_loss += torch.sqrt(loss).item()\n",
        "\n",
        "    if batch_idx % 200 == 199:\n",
        "      print('[batch: %d]  loss: %.3f'%(batch_idx+1, training_loss/200))\n",
        "      training_loss = 0\n",
        "\n",
        "def testing(model, test_loader, criterion):\n",
        "\n",
        "  model.eval()\n",
        "  predictions = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X_test, y_test in test_loader:\n",
        "\n",
        "      X_test = X_test.view(-1,len(X_test),len(X_test[0]))\n",
        "      \n",
        "      X_test = X_test.to(device)\n",
        "      \n",
        "      y_test = y_test.to(device)\n",
        "\n",
        "      output = model(X_test)\n",
        "\n",
        "      for pred in output.data.tolist():\n",
        "        predictions.append(pred)\n",
        "\n",
        "  testing_loss = np.sqrt(((np.array(predictions) - np.array(y_val)) ** 2).mean())\n",
        "  \n",
        "  pearson = pearsonr(y_val, predictions)\n",
        "\n",
        "  print(f'testing_loss: {testing_loss} Pearson {pearson[0]}')\n",
        "\n",
        "  return predictions, testing_loss, pearson[0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKquPLQiyJjN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def classification():\n",
        "  bs = 4\n",
        "  epochs = 8\n",
        "  iput_dim = 768\n",
        "  hidden_dim = 750\n",
        "\n",
        "  predictions = []\n",
        "  testing_loss = []\n",
        "  pearsons = []\n",
        "  \n",
        "  train_dat = TensorDataset(torch.tensor(X_train),torch.tensor(y_train))\n",
        "  test_dat = TensorDataset(torch.tensor(X_val),torch.tensor(y_val))\n",
        "\n",
        "  train_loader = DataLoader(train_dat, batch_size=bs, shuffle=True,num_workers=2)\n",
        "  val_loader = DataLoader(test_dat, batch_size=bs, shuffle=False)\n",
        "\n",
        "  #model = FeedForwardClassification().to(device)\n",
        "\n",
        "  model = LSTM(iput_dim,hidden_dim,bs).to(device)\n",
        "  \n",
        "  opt = torch.optim.Adam(model.parameters(),lr = 0.0001)\n",
        "  \n",
        "  criterion = nn.MSELoss()\n",
        "\n",
        "  for i in range(epochs):\n",
        "    print(\"Epoch: %d\"%(i+1))\n",
        "    print(\"-\"*30)\n",
        "    training(model,train_loader,criterion,opt)\n",
        "    predictions, loss, pearson = testing(model,val_loader,criterion)\n",
        "\n",
        "    testing_loss.append(loss)\n",
        "    pearsons.append(pearson)\n",
        "\n",
        "    print(\"-\"*30)\n",
        "\n",
        "\n",
        "  fig, (ax1,ax2) = plt.subplots(1,2)\n",
        "  ax1.plot(list(range(1,epochs+1)), testing_loss)\n",
        "  ax1.set(xlabel='epochs', ylabel='test loss')\n",
        "  ax2.plot(list(range(1,epochs+1)), pearsons)\n",
        "  ax2.set(xlabel='epochs', ylabel='test pearsons')\n",
        "  fig= plt.figure(figsize=(15,8))\n",
        "  fig.tight_layout(pad=3.0)\n",
        "  plt.show()\n",
        "  \n",
        "  # writeToFile(predictions)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e62a4ea2-84b7-4aae-867a-0a68fe13d1f0",
        "id": "Ml-mX7vSAfP_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "classification()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "------------------------------\n",
            "[batch: 200]  loss: 0.851\n",
            "[batch: 400]  loss: 0.815\n",
            "[batch: 600]  loss: 0.810\n",
            "[batch: 800]  loss: 0.780\n",
            "[batch: 1000]  loss: 0.821\n",
            "[batch: 1200]  loss: 0.790\n",
            "[batch: 1400]  loss: 0.807\n",
            "[batch: 1600]  loss: 0.749\n",
            "testing_loss: 0.8239563935156048 Pearson 0.436289873903415\n",
            "------------------------------\n",
            "Epoch: 2\n",
            "------------------------------\n",
            "[batch: 200]  loss: 0.751\n",
            "[batch: 400]  loss: 0.766\n",
            "[batch: 600]  loss: 0.770\n",
            "[batch: 800]  loss: 0.752\n",
            "[batch: 1000]  loss: 0.747\n",
            "[batch: 1200]  loss: 0.731\n",
            "[batch: 1400]  loss: 0.753\n",
            "[batch: 1600]  loss: 0.751\n",
            "testing_loss: 0.8309740450253066 Pearson 0.4394221075888252\n",
            "------------------------------\n",
            "Epoch: 3\n",
            "------------------------------\n",
            "[batch: 200]  loss: 0.757\n",
            "[batch: 400]  loss: 0.739\n",
            "[batch: 600]  loss: 0.733\n",
            "[batch: 800]  loss: 0.739\n",
            "[batch: 1000]  loss: 0.690\n",
            "[batch: 1200]  loss: 0.704\n",
            "[batch: 1400]  loss: 0.698\n",
            "[batch: 1600]  loss: 0.726\n",
            "testing_loss: 0.8203514617751535 Pearson 0.4409095796514726\n",
            "------------------------------\n",
            "Epoch: 4\n",
            "------------------------------\n",
            "[batch: 200]  loss: 0.714\n",
            "[batch: 400]  loss: 0.675\n",
            "[batch: 600]  loss: 0.660\n",
            "[batch: 800]  loss: 0.683\n",
            "[batch: 1000]  loss: 0.677\n",
            "[batch: 1200]  loss: 0.662\n",
            "[batch: 1400]  loss: 0.710\n",
            "[batch: 1600]  loss: 0.678\n",
            "testing_loss: 0.8404758843697641 Pearson 0.4371985517488864\n",
            "------------------------------\n",
            "Epoch: 5\n",
            "------------------------------\n",
            "[batch: 200]  loss: 0.606\n",
            "[batch: 400]  loss: 0.617\n",
            "[batch: 600]  loss: 0.629\n",
            "[batch: 800]  loss: 0.626\n",
            "[batch: 1000]  loss: 0.646\n",
            "[batch: 1200]  loss: 0.615\n",
            "[batch: 1400]  loss: 0.632\n",
            "[batch: 1600]  loss: 0.613\n",
            "testing_loss: 0.8378476827235957 Pearson 0.42153915651541035\n",
            "------------------------------\n",
            "Epoch: 6\n",
            "------------------------------\n",
            "[batch: 200]  loss: 0.571\n",
            "[batch: 400]  loss: 0.570\n",
            "[batch: 600]  loss: 0.538\n",
            "[batch: 800]  loss: 0.581\n",
            "[batch: 1000]  loss: 0.553\n",
            "[batch: 1200]  loss: 0.557\n",
            "[batch: 1400]  loss: 0.520\n",
            "[batch: 1600]  loss: 0.594\n",
            "testing_loss: 0.858025606970929 Pearson 0.40457555938373174\n",
            "------------------------------\n",
            "Epoch: 7\n",
            "------------------------------\n",
            "[batch: 200]  loss: 0.503\n",
            "[batch: 400]  loss: 0.473\n",
            "[batch: 600]  loss: 0.501\n",
            "[batch: 800]  loss: 0.503\n",
            "[batch: 1000]  loss: 0.471\n",
            "[batch: 1200]  loss: 0.482\n",
            "[batch: 1400]  loss: 0.498\n",
            "[batch: 1600]  loss: 0.506\n",
            "testing_loss: 0.8770802353965649 Pearson 0.39947244964127576\n",
            "------------------------------\n",
            "Epoch: 8\n",
            "------------------------------\n",
            "[batch: 200]  loss: 0.408\n",
            "[batch: 400]  loss: 0.404\n",
            "[batch: 600]  loss: 0.434\n",
            "[batch: 800]  loss: 0.435\n",
            "[batch: 1000]  loss: 0.420\n",
            "[batch: 1200]  loss: 0.442\n",
            "[batch: 1400]  loss: 0.426\n",
            "[batch: 1600]  loss: 0.451\n",
            "testing_loss: 0.8915549300658828 Pearson 0.38777655717999787\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x576 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdxgT-wo2jDt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}