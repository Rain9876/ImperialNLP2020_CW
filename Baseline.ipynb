{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Baseline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGjdfOqXU3jl",
        "colab_type": "code",
        "outputId": "2aa90fe3-f62e-4dac-b17b-244d0810d2ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "from os.path import exists\n",
        "\n",
        "if not exists('enzh_data.zip'):\n",
        "    !wget -O enzh_data.zip https://competitions.codalab.org/my/datasets/download/03e23bd7-8084-4542-997b-6a1ca6dd8a5f\n",
        "    !unzip enzh_data.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-27 15:19:29--  https://competitions.codalab.org/my/datasets/download/03e23bd7-8084-4542-997b-6a1ca6dd8a5f\n",
            "Resolving competitions.codalab.org (competitions.codalab.org)... 129.175.22.230\n",
            "Connecting to competitions.codalab.org (competitions.codalab.org)|129.175.22.230|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: https://newcodalab.lri.fr/prod-private/dataset_data_file/None/630ec/en-zh.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=1af2811eb18497f6b2ae2bb83e63f15a267cb390b09440c07312e1b4bb944722&X-Amz-Date=20200227T151930Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20200227%2Fnewcodalab%2Fs3%2Faws4_request [following]\n",
            "--2020-02-27 15:19:30--  https://newcodalab.lri.fr/prod-private/dataset_data_file/None/630ec/en-zh.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=1af2811eb18497f6b2ae2bb83e63f15a267cb390b09440c07312e1b4bb944722&X-Amz-Date=20200227T151930Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20200227%2Fnewcodalab%2Fs3%2Faws4_request\n",
            "Resolving newcodalab.lri.fr (newcodalab.lri.fr)... 129.175.15.11\n",
            "Connecting to newcodalab.lri.fr (newcodalab.lri.fr)|129.175.15.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 870893 (850K) [application/zip]\n",
            "Saving to: ‘enzh_data.zip’\n",
            "\n",
            "enzh_data.zip       100%[===================>] 850.48K  1.03MB/s    in 0.8s    \n",
            "\n",
            "2020-02-27 15:19:33 (1.03 MB/s) - ‘enzh_data.zip’ saved [870893/870893]\n",
            "\n",
            "Archive:  enzh_data.zip\n",
            "  inflating: dev.enzh.mt             \n",
            "  inflating: dev.enzh.scores         \n",
            "  inflating: dev.enzh.src            \n",
            "  inflating: test.enzh.mt            \n",
            "  inflating: test.enzh.src           \n",
            "  inflating: train.enzh.mt           \n",
            "  inflating: train.enzh.src          \n",
            "  inflating: train.enzh.scores       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D77IiOZcVPLq",
        "colab_type": "code",
        "outputId": "2fdaf0f0-b89c-4e87-cb43-75c16ee26118",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "#English-Chinese\n",
        "#Checking Data\n",
        "print(\"---EN-ZH---\")\n",
        "print()\n",
        "\n",
        "with open(\"./train.enzh.src\", \"r\") as enzh_src:\n",
        "  print(\"Source: \",enzh_src.readline())\n",
        "with open(\"./train.enzh.mt\", \"r\") as enzh_mt:\n",
        "  print(\"Translation: \",enzh_mt.readline())\n",
        "with open(\"./train.enzh.scores\", \"r\") as enzh_scores:\n",
        "  print(\"Score: \",enzh_scores.readline())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---EN-ZH---\n",
            "\n",
            "Source:  The last conquistador then rides on with his sword drawn.\n",
            "\n",
            "Translation:  最后的征服者骑着他的剑继续前进.\n",
            "\n",
            "Score:  -1.5284005772625449\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiDHWaJAVPsB",
        "colab_type": "code",
        "outputId": "a2b4a65b-da60-47c2-af64-55ab8d0f59d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "# DON'T RUN IF YOU ALREADY RAN IT IN THE ENGLISH-GERMAN SECTION\n",
        "# Downloading spacy models for english\n",
        "\n",
        "!spacy download en_core_web_md\n",
        "!spacy link en_core_web_md en300"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_md==2.1.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.1.0/en_core_web_md-2.1.0.tar.gz (95.4MB)\n",
            "\u001b[K     |████████████████████████████████| 95.4MB 1.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: en-core-web-md\n",
            "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-md: filename=en_core_web_md-2.1.0-cp36-none-any.whl size=97126236 sha256=ebc7a7e21e9086be16aa8c2249b70c7657bff60fcd77521897b4f3e9f8d5de31\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-0y5yz1jq/wheels/c1/2c/5f/fd7f3ec336bf97b0809c86264d2831c5dfb00fc2e239d1bb01\n",
            "Successfully built en-core-web-md\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-2.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_md -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en300\n",
            "You can now load the model via spacy.load('en300')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odOWt5kyVZc7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fa42b961-4107-4b7c-a22e-01f25f8e537c"
      },
      "source": [
        "import torchtext\n",
        "import spacy\n",
        "\n",
        "#Embeddings\n",
        "glove = torchtext.vocab.GloVe(name='6B', dim=100)\n",
        "\n",
        "#tokenizer model\n",
        "nlp_en =spacy.load('en300')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [06:25, 2.23MB/s]                          \n",
            "100%|█████████▉| 399069/400000 [00:41<00:00, 17530.80it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xclH0j2Tw1g0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # Example\n",
        "  marked_text_en = [\n",
        "              \"The last conquistador then rides on with his sword drawn.\",\n",
        "              \"He shoves Owen into the pit where Digger rips out his son's heart.\",\n",
        "              \"Alpha Phi Alpha also participates in the March of Dimes' WalkAmerica and raised over $181,000 in 2006.\",\n",
        "              \"In 1995, Deftones released their debut album Adrenaline.\",\n",
        "              \"Kyrgios also supports the North Melbourne Kangaroos Football Club in the Australian Football League.\"]\n",
        "  marked_text_zh = [\n",
        "            \"最后的征服者骑着他的剑继续前进.\",\n",
        "            \"他把欧文扔进了挖掘机挖出儿子心脏的坑里.\",\n",
        "            \"Alpha Phi Alpha 还参加了 Dimes WalkAmerica 的 3 月活动 ， 并在 2006 年筹集了 181 000 美元。\",\n",
        "            \"1995 年 ， Deftones 发行了首张专辑《肾上腺素》。\",\n",
        "            \"基尔吉奥斯还在澳大利亚足球联盟中支持北墨尔本袋鼠足球俱乐部.\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxs9zdVHVclc",
        "colab_type": "code",
        "outputId": "f55a2afa-252f-42ce-9bf2-f18a0c06267f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#ENGLISH EMBEDDINGS methods from the section GERMAN-ENGLISH\n",
        "# The difference from previous section is that we will use Glove embeddings directly because we are using a smaller model that spacy doesn't have\n",
        "# We add a method to compute the word embedding and a method to compute the sentence embedding by averaging the word vectors\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import nltk\n",
        "from nltk import download\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "#downloading stopwords from the nltk package\n",
        "download('stopwords') #stopwords dictionary, run once\n",
        "\n",
        "stop_words_en = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess(sentence,nlp):\n",
        "    text = sentence.lower()\n",
        "    doc = [token.lemma_ for token in  nlp.tokenizer(text)]\n",
        "    doc = [word for word in doc if word not in stop_words_en]\n",
        "    doc = [word for word in doc if word.isalpha()] #restricts string to alphabetic characters only\n",
        "    return doc\n",
        "\n",
        "\n",
        "def get_word_vector(embeddings, word):\n",
        "    try:\n",
        "      vec = embeddings.vectors[embeddings.stoi[word]]\n",
        "      return vec\n",
        "    except KeyError:\n",
        "      #print(f\"Word {word} does not exist\")\n",
        "      pass\n",
        "\n",
        "def get_sentence_vector(embeddings,line):\n",
        "  vectors = []\n",
        "  for w in line:\n",
        "    emb = get_word_vector(embeddings,w)\n",
        "    #do not add if the word is out of vocabulary\n",
        "    if emb is not None:\n",
        "      vectors.append(emb)\n",
        "  return torch.mean(torch.stack(vectors), axis = 0)\n",
        "\n",
        "\n",
        "def get_embeddings(f,embeddings,lang):\n",
        "  file = open(f) \n",
        "  lines = file.readlines() \n",
        "  sentences_vectors =[]\n",
        "\n",
        "  for l in lines:\n",
        "    sentence= preprocess(l,lang)\n",
        "    try:\n",
        "      vec = get_sentence_vector(embeddings,sentence)\n",
        "      sentences_vectors.append(vec)\n",
        "    except:\n",
        "      sentences_vectors.append(0)\n",
        "\n",
        "  return torch.stack(sentences_vectors)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAB-IDdfxBsV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# output = get_embeddings(marked_text_en,glove,nlp_en)\n",
        "\n",
        "# print(output.size())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBmYplZsagHs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Chinese"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jA2URyRzajGC",
        "colab_type": "code",
        "outputId": "fd16af87-3892-416b-8adb-5b0864b4e24e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "!wget -c https://github.com/Tony607/Chinese_sentiment_analysis/blob/master/data/chinese_stop_words.txt\n",
        "\n",
        "!wget -O zh.zip http://vectors.nlpl.eu/repository/20/35.zip\n",
        "\n",
        "!unzip zh.zip "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-27 15:27:43--  https://github.com/Tony607/Chinese_sentiment_analysis/blob/master/data/chinese_stop_words.txt\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘chinese_stop_words.txt’\n",
            "\n",
            "chinese_stop_words.     [  <=>               ] 417.14K  1.23MB/s    in 0.3s    \n",
            "\n",
            "2020-02-27 15:27:43 (1.23 MB/s) - ‘chinese_stop_words.txt’ saved [427156]\n",
            "\n",
            "--2020-02-27 15:27:45--  http://vectors.nlpl.eu/repository/20/35.zip\n",
            "Resolving vectors.nlpl.eu (vectors.nlpl.eu)... 129.240.189.225\n",
            "Connecting to vectors.nlpl.eu (vectors.nlpl.eu)|129.240.189.225|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1458485917 (1.4G) [application/zip]\n",
            "Saving to: ‘zh.zip’\n",
            "\n",
            "zh.zip              100%[===================>]   1.36G  17.2MB/s    in 88s     \n",
            "\n",
            "2020-02-27 15:29:13 (15.8 MB/s) - ‘zh.zip’ saved [1458485917/1458485917]\n",
            "\n",
            "Archive:  zh.zip\n",
            "  inflating: LIST                    \n",
            "  inflating: meta.json               \n",
            "  inflating: model.bin               \n",
            "  inflating: model.txt               \n",
            "  inflating: README                  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8JREt60ajvj",
        "colab_type": "code",
        "outputId": "bb612f58-29e1-43e3-ba9c-23ed16b5811d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "# from gensim.test.utils import datapath\n",
        "\n",
        "\n",
        "wv_from_bin = KeyedVectors.load_word2vec_format(\"model.bin\", binary=True) "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qD2OPE-iZ8b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "408b4d29-e059-481e-8af9-cda571e74345"
      },
      "source": [
        "\n",
        "import string\n",
        "import jieba\n",
        "import gensim \n",
        "import spacy\n",
        "import numpy as np\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stop_words = [line.rstrip() for line in open('./stopwords_ch.txt',\"r\", encoding=\"utf-8\") ]\n",
        "# for i in range(len(stop_words)):  \n",
        "  # print(stop_words[i])\n",
        "\n",
        "def get_sentence_vector_zh(line):\n",
        "  vectors = []\n",
        "  for w in line:\n",
        "    try:\n",
        "      emb = wv_from_bin[w]\n",
        "      vectors.append(emb)\n",
        "    except:\n",
        "      pass #Do not add if the word is out of vocabulary\n",
        "  if vectors:\n",
        "    vectors = torch.tensor(np.array(vectors))\n",
        "    return torch.mean(vectors,axis=0)  \n",
        "  else:\n",
        "    return None\n",
        "\n",
        "\n",
        "def processing_zh(sentence):\n",
        "  wordnet_lem = WordNetLemmatizer()\n",
        "  docs = []\n",
        "  seg_list = jieba.cut(sentence)\n",
        "  doc = [word for word in seg_list if word not in stop_words]\n",
        "  for token in doc:\n",
        "    if token.isalnum() and not token.isdigit():\n",
        "      if token.isalpha():\n",
        "        token=token.lower()\n",
        "        token = wordnet_lem.lemmatize(token, pos=\"v\")\n",
        "      docs.append(token)\n",
        "  # print(docs)\n",
        "  return docs\n",
        "\n",
        "\n",
        "def get_sentence_embeddings_zh(f):\n",
        "  file = open(f) \n",
        "  lines = file.readlines() \n",
        "  # lines = f\n",
        "  sentences_vectors =[]\n",
        "  for l in lines:\n",
        "    sent  = processing_zh(l)\n",
        "    vec = get_sentence_vector_zh(sent)\n",
        "    if vec is not None:\n",
        "      sentences_vectors.append(vec)\n",
        "    else:\n",
        "      sentences_vectors.append(torch.zeros(100))\n",
        "  return torch.stack(sentences_vectors)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqBCK3NHilrf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# output = get_sentence_embeddings_zh(marked_text_zh)\n",
        "\n",
        "# print(output.size())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KigM_lKLiqFL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "import torchtext\n",
        "from torchtext import data\n",
        "\n",
        "\n",
        "zh_train_mt = get_sentence_embeddings_zh(\"./train.enzh.mt\")\n",
        "zh_train_src = get_embeddings(\"./train.enzh.src\",glove,nlp_en)\n",
        "f_train_scores = open(\"./train.enzh.scores\",'r')\n",
        "zh_train_scores = f_train_scores.readlines()\n",
        "\n",
        "\n",
        "zh_val_src = get_embeddings(\"./dev.enzh.src\",glove,nlp_en)\n",
        "zh_val_mt = get_sentence_embeddings_zh(\"./dev.enzh.mt\")\n",
        "f_val_scores = open(\"./dev.enzh.scores\",'r')\n",
        "zh_val_scores = f_val_scores.readlines()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nhgqtsMnBq_",
        "colab_type": "code",
        "outputId": "6f767d8e-28e6-4c20-947e-97d6dccf844f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(f\"Training mt: {zh_train_mt.size()} Training src: {zh_train_src.size()}\")\n",
        "print()\n",
        "print(f\"Validation mt: {zh_val_mt.size()} Validation src: {zh_val_src.size()}\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training mt: torch.Size([7000, 100]) Training src: torch.Size([7000, 100])\n",
            "\n",
            "Validation mt: torch.Size([1000, 100]) Validation src: torch.Size([1000, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUtHdWTlnIJt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "71af7a4f-42e2-46e8-a2f3-c1d49782cc10"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "x_train = torch.cat((zh_train_src,zh_train_mt),1)\n",
        "x_val = torch.cat((zh_val_src,zh_val_mt),1)\n",
        "\n",
        "X_train_zh = np.array(x_train)\n",
        "X_val_zh = np.array(x_val)\n",
        "\n",
        "print(X_train_zh.shape)\n",
        "print(X_val_zh.shape)\n",
        "\n",
        "#Scores\n",
        "train_scores = np.array(zh_train_scores).astype(float)\n",
        "y_train_zh =train_scores\n",
        "\n",
        "val_scores = np.array(zh_val_scores).astype(float)\n",
        "y_val_zh =val_scores"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7000, 200)\n",
            "(1000, 200)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCQWDiA0nIfB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def rmse(predictions, targets):\n",
        "    return np.sqrt(((predictions - targets) ** 2).mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thtZFy7KUQZn",
        "colab_type": "code",
        "outputId": "60d1fa3d-7135-49d4-87ca-b267b498dcfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "from sklearn.svm import SVR\n",
        "from scipy.stats.stats import pearsonr\n",
        "\n",
        "for k in ['linear','poly','rbf','sigmoid']:\n",
        "    clf_t = SVR(kernel=k)\n",
        "    clf_t.fit(X_train_zh, y_train_zh)\n",
        "    print(k)\n",
        "    predictions = clf_t.predict(X_val_zh)\n",
        "    pearson = pearsonr(y_val_zh, predictions)\n",
        "    print(f'RMSE: {rmse(predictions,y_val_zh)} Pearson {pearson[0]}')\n",
        "    print()\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "linear\n",
            "RMSE: 0.9169253579770051 Pearson 0.2634595391428125\n",
            "\n",
            "poly\n",
            "RMSE: 0.9075232411468231 Pearson 0.2782473419325568\n",
            "\n",
            "rbf\n",
            "RMSE: 0.9041666243501958 Pearson 0.29848876852419753\n",
            "\n",
            "sigmoid\n",
            "RMSE: 5.667260753284455 Pearson -0.052260479449302864\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoxNBs50UQ3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the model we are using\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "rf = RandomForestRegressor(n_estimators = 1000, random_state = 666)\n",
        "rf.fit(X_train_zh, y_train_zh);\n",
        "predictions = rf.predict(X_val_zh)\n",
        "\n",
        "pearson = pearsonr(y_val_zh, predictions)\n",
        "print('RMSE:', rmse(predictions,y_val_zh))\n",
        "print(f\"Pearson {pearson[0]}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AM1RIYTLOgmK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.stats.stats import pearsonr\n",
        "from os.path import exists\n",
        "import numpy as np\n",
        "from sklearn import linear_model\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.optim.adam\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUW_rgO7OyHj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "f27a4544-d87d-4ea7-ff9a-479be0aa653f"
      },
      "source": [
        "import torch\n",
        "use_GPU = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_GPU else \"cpu\")\n",
        "print('Device: ' + str(device))\n",
        "if use_GPU:\n",
        "    torch.cuda.manual_seed(0)\n",
        "    print('GPU: ' + str(torch.cuda.get_device_name(int(\"0\")))) \n",
        "print(\"Using GPU: {}\".format(use_GPU))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "GPU: Tesla P100-PCIE-16GB\n",
            "Using GPU: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jePBgALHNR8X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# FFNN\n",
        "class FeedForwardClassification(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(768,400)\n",
        "        self.fc2 = nn.Linear(400,100)\n",
        "        self.fc3 = nn.Linear(100,1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.fc3(F.relu(self.fc2(F.relu(self.fc1(x)))))\n",
        "        return output\n",
        "\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, batch_size, output_dim=1,num_layers=2):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.batch_size = batch_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers)\n",
        "        self.fc1 = nn.Linear(self.hidden_dim, 1)\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
        "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
        "\n",
        "    def forward(self, input):\n",
        "        # Forward pass through LSTM layer\n",
        "        # shape of lstm_out: [input_size, batch_size, hidden_dim]\n",
        "        # shape of self.hidden: (a, b), where a and b both \n",
        "        # have shape (num_layers, batch_size, hidden_dim).\n",
        "        lstm_out, self.hidden = self.lstm(input.view(len(input), self.batch_size, -1))\n",
        "        \n",
        "        # Only take the output from the final timetep\n",
        "        # Can pass on the entirety of lstm_out to the next layer if it is a seq2seq prediction\n",
        "        y_pred = self.fc1(lstm_out[-1].view(self.batch_size, -1))\n",
        "\n",
        "\n",
        "        return y_pred.view(-1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ve00D-JLNcRc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def training(model,train_loader, criterion, opt):\n",
        "  training_loss = 0\n",
        "  model.train()\n",
        "\n",
        "  for batch_idx,(X_train, y_train) in enumerate(train_loader):\n",
        "    \n",
        "\n",
        "    X_train = X_train.view(-1,len(X_train),len(X_train[0]))\n",
        "\n",
        "    opt.zero_grad()\n",
        "\n",
        "    X_train = X_train.to(device)\n",
        "    y_train = y_train.to(device)\n",
        "\n",
        "    output = model(X_train)\n",
        "    \n",
        "    loss = criterion(output,y_train)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    opt.step()\n",
        "\n",
        "    training_loss += torch.sqrt(loss).item()\n",
        "\n",
        "    if batch_idx % 200 == 199:\n",
        "      print('[batch: %d]  loss: %.3f'%(batch_idx+1, training_loss/200))\n",
        "      training_loss = 0\n",
        "\n",
        "def testing(model, test_loader, criterion):\n",
        "\n",
        "  model.eval()\n",
        "  predictions = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X_test, y_test in test_loader:\n",
        "\n",
        "      X_test = X_test.view(-1,len(X_test),len(X_test[0]))\n",
        "      \n",
        "      X_test = X_test.to(device)\n",
        "      \n",
        "      y_test = y_test.to(device)\n",
        "\n",
        "      output = model(X_test)\n",
        "\n",
        "      for pred in output.data.tolist():\n",
        "        predictions.append(pred)\n",
        "\n",
        "  testing_loss = np.sqrt(((np.array(predictions) - np.array(y_val_zh)) ** 2).mean())\n",
        "  \n",
        "  pearson = pearsonr(y_val_zh, predictions)\n",
        "\n",
        "  print(f'testing_loss: {testing_loss} Pearson {pearson[0]}')\n",
        "\n",
        "  return predictions, testing_loss, pearson[0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QM-OaU_NesX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def regression():\n",
        "  bs = 4\n",
        "  epochs = 20\n",
        "  iput_dim = 200\n",
        "  hidden_dim = 750\n",
        "\n",
        "  predictions = []\n",
        "  testing_loss = []\n",
        "  pearsons = []\n",
        "  \n",
        "  train_dat = TensorDataset(torch.FloatTensor(x_train),torch.FloatTensor(y_train_zh))\n",
        "  test_dat = TensorDataset(torch.FloatTensor(x_val),torch.FloatTensor(y_val_zh))\n",
        "\n",
        "  train_loader = DataLoader(train_dat, batch_size=bs, shuffle=True,num_workers=2)\n",
        "  val_loader = DataLoader(test_dat, batch_size=bs, shuffle=False)\n",
        "\n",
        "  #model = FeedForwardClassification().to(device)\n",
        "\n",
        "  model = LSTM(iput_dim,hidden_dim,bs).to(device)\n",
        "  \n",
        "  opt = torch.optim.Adam(model.parameters(),lr = 0.0001,weight_decay=1e-6)\n",
        "  \n",
        "  criterion = nn.MSELoss()\n",
        "\n",
        "  for i in range(epochs):\n",
        "    print(\"Epoch: %d\"%(i+1))\n",
        "    print(\"-\"*30)\n",
        "    training(model,train_loader,criterion,opt)\n",
        "    predictions, loss, pearson = testing(model,val_loader,criterion)\n",
        "\n",
        "    testing_loss.append(loss)\n",
        "    pearsons.append(pearson)\n",
        "\n",
        "    print(\"-\"*30)\n",
        "\n",
        "  x_epochs = list(range(1,epochs+1))\n",
        "  fig, (ax1,ax2) = plt.subplots(1,2)\n",
        "  ax1.plot(x_epochs, testing_loss)\n",
        "  ax1.set(xlabel='epochs', ylabel='test loss')\n",
        "  ax2.plot(x_epochs, pearsons)\n",
        "  ax2.set(xlabel='epochs', ylabel='test pearsons')\n",
        "  ax1.set_xticks(x_epochs) \n",
        "  ax2.set_xticks(x_epochs) \n",
        "  fig.tight_layout(pad=4.0)\n",
        "  plt.show()\n",
        "  \n",
        "  # return testing_loss, pearsons\n",
        "  # writeToFile(predictions)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "087DdPK5Nf5G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f9435117-be82-4341-d91a-f590fa628b90"
      },
      "source": [
        "regression()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "------------------------------\n",
            "[batch: 200]  loss: 0.850\n",
            "[batch: 400]  loss: 0.855\n",
            "[batch: 600]  loss: 0.803\n",
            "[batch: 800]  loss: 0.861\n",
            "[batch: 1000]  loss: 0.820\n",
            "[batch: 1200]  loss: 0.827\n",
            "[batch: 1400]  loss: 0.828\n",
            "[batch: 1600]  loss: 0.824\n",
            "testing_loss: 0.8691827209464427 Pearson 0.3031952170097853\n",
            "------------------------------\n",
            "Epoch: 2\n",
            "------------------------------\n",
            "[batch: 200]  loss: 0.812\n",
            "[batch: 400]  loss: 0.772\n",
            "[batch: 600]  loss: 0.838\n",
            "[batch: 800]  loss: 0.827\n",
            "[batch: 1000]  loss: 0.857\n",
            "[batch: 1200]  loss: 0.828\n",
            "[batch: 1400]  loss: 0.805\n",
            "[batch: 1600]  loss: 0.815\n",
            "testing_loss: 0.8673364342923324 Pearson 0.30500494314001014\n",
            "------------------------------\n",
            "Epoch: 3\n",
            "------------------------------\n",
            "[batch: 200]  loss: 0.814\n",
            "[batch: 400]  loss: 0.794\n",
            "[batch: 600]  loss: 0.815\n",
            "[batch: 800]  loss: 0.817\n",
            "[batch: 1000]  loss: 0.844\n",
            "[batch: 1200]  loss: 0.792\n",
            "[batch: 1400]  loss: 0.812\n",
            "[batch: 1600]  loss: 0.821\n",
            "testing_loss: 0.8788112330098985 Pearson 0.3013970697522002\n",
            "------------------------------\n",
            "Epoch: 4\n",
            "------------------------------\n",
            "[batch: 200]  loss: 0.811\n",
            "[batch: 400]  loss: 0.850\n",
            "[batch: 600]  loss: 0.773\n",
            "[batch: 800]  loss: 0.763\n",
            "[batch: 1000]  loss: 0.830\n",
            "[batch: 1200]  loss: 0.816\n",
            "[batch: 1400]  loss: 0.826\n",
            "[batch: 1600]  loss: 0.807\n",
            "testing_loss: 0.8645861264240108 Pearson 0.3101646164563227\n",
            "------------------------------\n",
            "Epoch: 5\n",
            "------------------------------\n",
            "[batch: 200]  loss: 0.777\n",
            "[batch: 400]  loss: 0.749\n",
            "[batch: 600]  loss: 0.783\n",
            "[batch: 800]  loss: 0.803\n",
            "[batch: 1000]  loss: 0.854\n",
            "[batch: 1200]  loss: 0.820\n",
            "[batch: 1400]  loss: 0.829\n",
            "[batch: 1600]  loss: 0.809\n",
            "testing_loss: 0.8661489727355439 Pearson 0.30572853379096054\n",
            "------------------------------\n",
            "Epoch: 6\n",
            "------------------------------\n",
            "[batch: 200]  loss: 0.804\n",
            "[batch: 400]  loss: 0.786\n",
            "[batch: 600]  loss: 0.847\n",
            "[batch: 800]  loss: 0.784\n",
            "[batch: 1000]  loss: 0.793\n",
            "[batch: 1200]  loss: 0.827\n",
            "[batch: 1400]  loss: 0.749\n",
            "[batch: 1600]  loss: 0.843\n",
            "testing_loss: 0.8648848063153143 Pearson 0.3111239877584189\n",
            "------------------------------\n",
            "Epoch: 7\n",
            "------------------------------\n",
            "[batch: 200]  loss: 0.802\n",
            "[batch: 400]  loss: 0.816\n",
            "[batch: 600]  loss: 0.806\n",
            "[batch: 800]  loss: 0.790\n",
            "[batch: 1000]  loss: 0.778\n",
            "[batch: 1200]  loss: 0.844\n",
            "[batch: 1400]  loss: 0.771\n",
            "[batch: 1600]  loss: 0.810\n",
            "testing_loss: 0.8678638345316475 Pearson 0.3058969407385939\n",
            "------------------------------\n",
            "Epoch: 8\n",
            "------------------------------\n",
            "[batch: 200]  loss: 0.836\n",
            "[batch: 400]  loss: 0.764\n",
            "[batch: 600]  loss: 0.804\n",
            "[batch: 800]  loss: 0.804\n",
            "[batch: 1000]  loss: 0.803\n",
            "[batch: 1200]  loss: 0.801\n",
            "[batch: 1400]  loss: 0.759\n",
            "[batch: 1600]  loss: 0.796\n",
            "testing_loss: 0.8667223047618877 Pearson 0.3071150776965039\n",
            "------------------------------\n",
            "Epoch: 9\n",
            "------------------------------\n",
            "[batch: 200]  loss: 0.751\n",
            "[batch: 400]  loss: 0.794\n",
            "[batch: 600]  loss: 0.766\n",
            "[batch: 800]  loss: 0.815\n",
            "[batch: 1000]  loss: 0.784\n",
            "[batch: 1200]  loss: 0.804\n",
            "[batch: 1400]  loss: 0.816\n",
            "[batch: 1600]  loss: 0.792\n",
            "testing_loss: 0.8672263955548051 Pearson 0.3093392150314839\n",
            "------------------------------\n",
            "Epoch: 10\n",
            "------------------------------\n",
            "[batch: 200]  loss: 0.754\n",
            "[batch: 400]  loss: 0.819\n",
            "[batch: 600]  loss: 0.773\n",
            "[batch: 800]  loss: 0.771\n",
            "[batch: 1000]  loss: 0.758\n",
            "[batch: 1200]  loss: 0.795\n",
            "[batch: 1400]  loss: 0.800\n",
            "[batch: 1600]  loss: 0.831\n",
            "testing_loss: 0.8685727163140714 Pearson 0.31175316609802234\n",
            "------------------------------\n",
            "Epoch: 11\n",
            "------------------------------\n",
            "[batch: 200]  loss: 0.812\n",
            "[batch: 400]  loss: 0.788\n",
            "[batch: 600]  loss: 0.793\n",
            "[batch: 800]  loss: 0.785\n",
            "[batch: 1000]  loss: 0.777\n",
            "[batch: 1200]  loss: 0.803\n",
            "[batch: 1400]  loss: 0.758\n",
            "[batch: 1600]  loss: 0.733\n",
            "testing_loss: 0.8719554284079216 Pearson 0.310866958924463\n",
            "------------------------------\n",
            "Epoch: 12\n",
            "------------------------------\n",
            "[batch: 200]  loss: 0.832\n",
            "[batch: 400]  loss: 0.755\n",
            "[batch: 600]  loss: 0.751\n",
            "[batch: 800]  loss: 0.801\n",
            "[batch: 1000]  loss: 0.776\n",
            "[batch: 1200]  loss: 0.781\n",
            "[batch: 1400]  loss: 0.755\n",
            "[batch: 1600]  loss: 0.773\n",
            "testing_loss: 0.8804150472655815 Pearson 0.296065286621663\n",
            "------------------------------\n",
            "Epoch: 13\n",
            "------------------------------\n",
            "[batch: 200]  loss: 0.761\n",
            "[batch: 400]  loss: 0.779\n",
            "[batch: 600]  loss: 0.783\n",
            "[batch: 800]  loss: 0.790\n",
            "[batch: 1000]  loss: 0.801\n",
            "[batch: 1200]  loss: 0.786\n",
            "[batch: 1400]  loss: 0.747\n",
            "[batch: 1600]  loss: 0.768\n",
            "testing_loss: 0.8723241454786433 Pearson 0.2959330053136825\n",
            "------------------------------\n",
            "Epoch: 14\n",
            "------------------------------\n",
            "[batch: 200]  loss: 0.755\n",
            "[batch: 400]  loss: 0.769\n",
            "[batch: 600]  loss: 0.772\n",
            "[batch: 800]  loss: 0.770\n",
            "[batch: 1000]  loss: 0.772\n",
            "[batch: 1200]  loss: 0.742\n",
            "[batch: 1400]  loss: 0.788\n",
            "[batch: 1600]  loss: 0.751\n",
            "testing_loss: 0.8752371569315781 Pearson 0.2901495979578805\n",
            "------------------------------\n",
            "Epoch: 15\n",
            "------------------------------\n",
            "[batch: 200]  loss: 0.728\n",
            "[batch: 400]  loss: 0.735\n",
            "[batch: 600]  loss: 0.766\n",
            "[batch: 800]  loss: 0.772\n",
            "[batch: 1000]  loss: 0.716\n",
            "[batch: 1200]  loss: 0.744\n",
            "[batch: 1400]  loss: 0.820\n",
            "[batch: 1600]  loss: 0.797\n",
            "testing_loss: 0.888193033309579 Pearson 0.2859984167400407\n",
            "------------------------------\n",
            "Epoch: 16\n",
            "------------------------------\n",
            "[batch: 200]  loss: 0.784\n",
            "[batch: 400]  loss: 0.764\n",
            "[batch: 600]  loss: 0.765\n",
            "[batch: 800]  loss: 0.809\n",
            "[batch: 1000]  loss: 0.727\n",
            "[batch: 1200]  loss: 0.760\n",
            "[batch: 1400]  loss: 0.745\n",
            "[batch: 1600]  loss: 0.737\n",
            "testing_loss: 0.8815738035850303 Pearson 0.29026883200075004\n",
            "------------------------------\n",
            "Epoch: 17\n",
            "------------------------------\n",
            "[batch: 200]  loss: 0.764\n",
            "[batch: 400]  loss: 0.745\n",
            "[batch: 600]  loss: 0.757\n",
            "[batch: 800]  loss: 0.728\n",
            "[batch: 1000]  loss: 0.729\n",
            "[batch: 1200]  loss: 0.732\n",
            "[batch: 1400]  loss: 0.750\n",
            "[batch: 1600]  loss: 0.784\n",
            "testing_loss: 0.8815020754877563 Pearson 0.2804702562904664\n",
            "------------------------------\n",
            "Epoch: 18\n",
            "------------------------------\n",
            "[batch: 200]  loss: 0.750\n",
            "[batch: 400]  loss: 0.709\n",
            "[batch: 600]  loss: 0.720\n",
            "[batch: 800]  loss: 0.756\n",
            "[batch: 1000]  loss: 0.769\n",
            "[batch: 1200]  loss: 0.764\n",
            "[batch: 1400]  loss: 0.753\n",
            "[batch: 1600]  loss: 0.744\n",
            "testing_loss: 0.884283582460199 Pearson 0.283485722451971\n",
            "------------------------------\n",
            "Epoch: 19\n",
            "------------------------------\n",
            "[batch: 200]  loss: 0.705\n",
            "[batch: 400]  loss: 0.706\n",
            "[batch: 600]  loss: 0.707\n",
            "[batch: 800]  loss: 0.734\n",
            "[batch: 1000]  loss: 0.750\n",
            "[batch: 1200]  loss: 0.714\n",
            "[batch: 1400]  loss: 0.737\n",
            "[batch: 1600]  loss: 0.793\n",
            "testing_loss: 0.8903389899146776 Pearson 0.27722243575157374\n",
            "------------------------------\n",
            "Epoch: 20\n",
            "------------------------------\n",
            "[batch: 200]  loss: 0.736\n",
            "[batch: 400]  loss: 0.726\n",
            "[batch: 600]  loss: 0.712\n",
            "[batch: 800]  loss: 0.739\n",
            "[batch: 1000]  loss: 0.732\n",
            "[batch: 1200]  loss: 0.714\n",
            "[batch: 1400]  loss: 0.729\n",
            "[batch: 1600]  loss: 0.787\n",
            "testing_loss: 0.8850257464659056 Pearson 0.2756166374263927\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAADeCAYAAADy3YFwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHsqrgAXUS1X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "def writeScores(method_name,scores):\n",
        "    fn = \"predictions.txt\"\n",
        "    print(\"\")\n",
        "    with open(fn, 'w') as output_file:\n",
        "        for idx,x in enumerate(scores):\n",
        "            #out =  metrics[idx]+\":\"+str(\"{0:.2f}\".format(x))+\"\\n\"\n",
        "            #print(out)\n",
        "            output_file.write(f\"{x}\\n\")\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUuUtX8cUVkB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#EN_ZH\n",
        "\n",
        "zh_test_mt = get_sentence_embeddings_zh(\"./test.enzh.mt\")\n",
        "zh_test_src = get_embeddings(\"./test.enzh.src\",glove,nlp_en)\n",
        "\n",
        "X= [np.array(zh_test_mt),np.array(zh_test_src)]\n",
        "X_test_zh = np.array(X).transpose()\n",
        "\n",
        "#Predict\n",
        "clf_zh = SVR(kernel='rbf')\n",
        "clf_zh.fit(X_train_zh, y_train_zh)\n",
        "\n",
        "predictions_zh = clf_zh.predict(X_test_zh)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnxD_loCUYF7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#EN_ZH\n",
        "\n",
        "from google.colab import files\n",
        "from zipfile import ZipFile\n",
        "\n",
        "\n",
        "writeScores(\"SVR\",predictions_zh)\n",
        "\n",
        "with ZipFile(\"en-zh_svr.zip\",\"w\") as newzip:\n",
        "\tnewzip.write(\"predictions.txt\")\n",
        " \n",
        "files.download('en-zh_svr.zip') \n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}